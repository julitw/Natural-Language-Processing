{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter-Efficient Fine-Tuning (PEFT) of BERT base model to predict medical diagnosis - tutorial \n",
    "https://medium.com/@nubyra/parameter-efficient-fine-tuning-peft-of-bert-base-model-to-predict-medical-diagnosis-5086a1828f4b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Python\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline, DataCollatorWithPadding, Trainer, TrainingArguments, BertForSequenceClassification, pipeline\n",
    "from peft import PeftModel, PeftConfig, LoraConfig, TaskType, get_peft_model\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load medical diagnosis dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['label', 'input_text'],\n",
      "        num_rows: 853\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['label', 'input_text'],\n",
      "        num_rows: 212\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "data_files = {\"train\": \"train.jsonl\", \"test\": \"test.jsonl\"}\n",
    "dataset = load_dataset(\"gretelai/symptom_to_diagnosis\", data_files=data_files)\n",
    "dataset = dataset.rename_column(\"output_text\", \"label\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT: I've been having a lot of pain in my neck and back. I've also been having trouble with my balance and coordination. I've been coughing a lot and my limbs feel weak. \n",
      "OUTPUT: cervical spondylosis\n",
      "\n",
      "INPUT: I have a rash on my face that is getting worse. It is red, inflamed, and has blisters that are bleeding clear pus. It is really painful. \n",
      "OUTPUT: impetigo\n",
      "\n",
      "INPUT: I have been urinating blood. I sometimes feel sick to my stomach when I urinate. I often feel like I have a fever. \n",
      "OUTPUT: urinary tract infection\n",
      "\n",
      "INPUT: I have been having trouble with my muscles and joints. My neck is really tight and my muscles feel weak. I have swollen joints and it is hard to move around without becoming stiff. It is also really uncomfortable to walk. \n",
      "OUTPUT: arthritis\n",
      "\n",
      "INPUT: I have been feeling really sick. My body hurts a lot and I have no appetite. I have also developed rashes on my arms and face. The back of my eyes hurt a lot. \n",
      "OUTPUT: dengue\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for entry in dataset['train'].select(range(5)):\n",
    "    print('INPUT: {} \\nOUTPUT: {}\\n'.format(entry['input_text'], entry['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Diagnosis</th>\n",
       "      <th>train_set</th>\n",
       "      <th>test_set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>allergy</td>\n",
       "      <td>40</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>arthritis</td>\n",
       "      <td>40</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bronchial asthma</td>\n",
       "      <td>40</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cervical spondylosis</td>\n",
       "      <td>40</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chicken pox</td>\n",
       "      <td>40</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>common cold</td>\n",
       "      <td>39</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>dengue</td>\n",
       "      <td>40</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>diabetes</td>\n",
       "      <td>40</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>drug reaction</td>\n",
       "      <td>40</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>fungal infection</td>\n",
       "      <td>39</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>gastroesophageal reflux disease</td>\n",
       "      <td>39</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>hypertension</td>\n",
       "      <td>40</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>impetigo</td>\n",
       "      <td>40</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>jaundice</td>\n",
       "      <td>33</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>malaria</td>\n",
       "      <td>40</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>migraine</td>\n",
       "      <td>32</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>peptic ulcer disease</td>\n",
       "      <td>37</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>pneumonia</td>\n",
       "      <td>37</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>psoriasis</td>\n",
       "      <td>40</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>typhoid</td>\n",
       "      <td>38</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>urinary tract infection</td>\n",
       "      <td>39</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>varicose veins</td>\n",
       "      <td>40</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Diagnosis  train_set  test_set\n",
       "0                           allergy         40        10\n",
       "1                         arthritis         40        10\n",
       "2                  bronchial asthma         40        10\n",
       "3              cervical spondylosis         40        10\n",
       "4                       chicken pox         40        10\n",
       "5                       common cold         39        10\n",
       "6                            dengue         40        10\n",
       "7                          diabetes         40        10\n",
       "8                     drug reaction         40         8\n",
       "9                  fungal infection         39         9\n",
       "10  gastroesophageal reflux disease         39        10\n",
       "11                     hypertension         40        10\n",
       "12                         impetigo         40        10\n",
       "13                         jaundice         33         7\n",
       "14                          malaria         40        10\n",
       "15                         migraine         32        10\n",
       "16             peptic ulcer disease         37        10\n",
       "17                        pneumonia         37        10\n",
       "18                        psoriasis         40        10\n",
       "19                          typhoid         38         9\n",
       "20          urinary tract infection         39         9\n",
       "21                   varicose veins         40        10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_counts = pd.DataFrame({'Diagnosis': dataset['train']['label']})\n",
    "train_counts = train_counts.groupby('Diagnosis').size().reset_index(name='train_set')\n",
    "\n",
    "test_counts = pd.DataFrame({'Diagnosis': dataset['test']['label']})\n",
    "test_counts = test_counts.groupby('Diagnosis').size().reset_index(name='test_set')\n",
    "\n",
    "display(train_counts.merge(test_counts, on='Diagnosis'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction using Pre-trained BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f72966570af4e70ae1d407e6aa05000",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  50%|####9     | 220M/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "sorted_labels = sorted(set(dataset['train']['label']))\n",
    "label2id = dict(zip(sorted_labels, range(0, len(sorted_labels))))\n",
    "id2label = dict(zip(range(0, len(sorted_labels)), sorted_labels))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "foundation_model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
    "                                                                    num_labels=len(label2id),\n",
    "                                                                    label2id=label2id,\n",
    "                                                                    id2label=id2label)\n",
    "\n",
    "classifier = pipeline(\"text-classification\", model=foundation_model, tokenizer=tokenizer)\n",
    "predicted_labels = classifier(dataset['test']['input_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Foundation Model Accuracy: 7.08%\n"
     ]
    }
   ],
   "source": [
    "test_array = np.asarray(dataset['test']['label'])\n",
    "pred_array = np.asarray([item['label'] for item in predicted_labels])\n",
    "foundation_accuracy = round(sum(test_array == pred_array)*100/len(test_array), 2)\n",
    "print(f\"Foundation Model Accuracy: {foundation_accuracy}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PEFT-LORA configuration for BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSdpaSelfAttention(\n",
      "            (query): lora.Linear(\n",
      "              (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (lora_dropout): ModuleDict(\n",
      "                (default): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (lora_A): ModuleDict(\n",
      "                (default): Linear(in_features=768, out_features=64, bias=False)\n",
      "              )\n",
      "              (lora_B): ModuleDict(\n",
      "                (default): Linear(in_features=64, out_features=768, bias=False)\n",
      "              )\n",
      "              (lora_embedding_A): ParameterDict()\n",
      "              (lora_embedding_B): ParameterDict()\n",
      "              (lora_magnitude_vector): ModuleDict()\n",
      "            )\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): lora.Linear(\n",
      "              (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (lora_dropout): ModuleDict(\n",
      "                (default): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (lora_A): ModuleDict(\n",
      "                (default): Linear(in_features=768, out_features=64, bias=False)\n",
      "              )\n",
      "              (lora_B): ModuleDict(\n",
      "                (default): Linear(in_features=64, out_features=768, bias=False)\n",
      "              )\n",
      "              (lora_embedding_A): ParameterDict()\n",
      "              (lora_embedding_B): ParameterDict()\n",
      "              (lora_magnitude_vector): ModuleDict()\n",
      "            )\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS, r=64, lora_alpha=1, lora_dropout=0.1\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(foundation_model, lora_config)\n",
    "print(peft_model.bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,376,214 || all params: 111,875,372 || trainable%: 2.1240\n"
     ]
    }
   ],
   "source": [
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with PEFT-BERT model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess train/test data with appropriate tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8a0c732d38042c4b3985a726026174a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/853 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af4ce7c806eb4f5cb253ceaf7cba1b4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/212 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': Dataset({\n",
      "    features: ['label', 'input_text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 853\n",
      "}), 'test': Dataset({\n",
      "    features: ['label', 'input_text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 212\n",
      "})}\n"
     ]
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    \"\"\"Preprocess the dataset by returning tokenized examples.\"\"\"\n",
    "    tokens = tokenizer(examples[\"input_text\"], padding=\"max_length\", truncation=True)\n",
    "    tokens['label'] = [label2id[l] for l in examples[\"label\"]]\n",
    "    return tokens\n",
    "\n",
    "splits = ['train', 'test']\n",
    "\n",
    "tokenized_ds = {} \n",
    "\n",
    "for split in splits:\n",
    "    tokenized_ds[split] = dataset[split].map(preprocess_function, batched=True)\n",
    "\n",
    "print(tokenized_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A tokenized training input example:\n",
      "[101, 1045, 1005, 2310, 2042, 2383, 1037, 2843, 1997, 3255, 1999, 2026, 3300, 1998, 2067, 1012, 1045, 1005, 2310, 2036, 2042, 2383, 4390, 2007, 2026, 5703, 1998, 12016, 1012, 1045, 1005, 2310, 2042, 21454, 1037, 2843, 1998, 2026, 10726, 2514, 5410, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "\n",
      "A tokenized training label example:\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(\"A tokenized training input example:\")\n",
    "print(tokenized_ds[\"train\"][0][\"input_ids\"])\n",
    "print(\"\\n\")\n",
    "print(\"A tokenized training label example:\")\n",
    "print(tokenized_ds[\"train\"][0][\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train PEFT-BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\Lib\\site-packages\\transformers\\training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_19152\\3862884758.py:9: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to train...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecf9224298ed46d8bb9755fe54031171",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3210 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6554f14130094c00b4638766e688c05c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.2217342853546143, 'eval_accuracy': 28.77358490566038, 'eval_runtime': 7.5106, 'eval_samples_per_second': 28.227, 'eval_steps_per_second': 7.057, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0804d29ca5b54980afa52a8d7cc66d65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8072425127029419, 'eval_accuracy': 73.11320754716981, 'eval_runtime': 7.5486, 'eval_samples_per_second': 28.085, 'eval_steps_per_second': 7.021, 'epoch': 2.0}\n",
      "{'loss': 2.0961, 'grad_norm': 4.281309127807617, 'learning_rate': 0.001688473520249221, 'epoch': 2.34}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86be6dfe62eb481295491164d64e5e80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6400379538536072, 'eval_accuracy': 79.24528301886792, 'eval_runtime': 7.8099, 'eval_samples_per_second': 27.145, 'eval_steps_per_second': 6.786, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53744e6a6ffd4b119028e2075a5c3cf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.47103190422058105, 'eval_accuracy': 87.73584905660378, 'eval_runtime': 7.6676, 'eval_samples_per_second': 27.649, 'eval_steps_per_second': 6.912, 'epoch': 4.0}\n",
      "{'loss': 0.504, 'grad_norm': 2.1039960384368896, 'learning_rate': 0.0013769470404984424, 'epoch': 4.67}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a13db940fd2a4a4eaacaff46504b2638",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2670034170150757, 'eval_accuracy': 91.50943396226415, 'eval_runtime': 7.5753, 'eval_samples_per_second': 27.986, 'eval_steps_per_second': 6.996, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5641639d90e4ed3a0b79e82b8664337",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.37952834367752075, 'eval_accuracy': 88.67924528301887, 'eval_runtime': 7.5986, 'eval_samples_per_second': 27.9, 'eval_steps_per_second': 6.975, 'epoch': 6.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93a479a3c1eb4592982b9e6d1e8be8e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.25924569368362427, 'eval_accuracy': 91.50943396226415, 'eval_runtime': 7.6764, 'eval_samples_per_second': 27.617, 'eval_steps_per_second': 6.904, 'epoch': 7.0}\n",
      "{'loss': 0.1809, 'grad_norm': 2.9897329807281494, 'learning_rate': 0.0010654205607476634, 'epoch': 7.01}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d2c12e444a74a0a938adf7c33be7052",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.274662047624588, 'eval_accuracy': 92.45283018867924, 'eval_runtime': 7.6683, 'eval_samples_per_second': 27.646, 'eval_steps_per_second': 6.912, 'epoch': 8.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8dd8b574bdb49478c15a776d7dc8684",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.15629740059375763, 'eval_accuracy': 95.28301886792453, 'eval_runtime': 7.751, 'eval_samples_per_second': 27.351, 'eval_steps_per_second': 6.838, 'epoch': 9.0}\n",
      "{'loss': 0.0618, 'grad_norm': 0.28020086884498596, 'learning_rate': 0.0007538940809968847, 'epoch': 9.35}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "924ac3c17eed4766b68558472bb8f495",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.17917802929878235, 'eval_accuracy': 95.75471698113208, 'eval_runtime': 7.6844, 'eval_samples_per_second': 27.588, 'eval_steps_per_second': 6.897, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6dbcf97c3f54a7da397648a99862b7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.23992441594600677, 'eval_accuracy': 93.86792452830188, 'eval_runtime': 7.7859, 'eval_samples_per_second': 27.229, 'eval_steps_per_second': 6.807, 'epoch': 11.0}\n",
      "{'loss': 0.0221, 'grad_norm': 0.061193205416202545, 'learning_rate': 0.0004423676012461059, 'epoch': 11.68}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20fdbb77803f4480aeca7ff806a22fe5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.17434239387512207, 'eval_accuracy': 95.75471698113208, 'eval_runtime': 7.8977, 'eval_samples_per_second': 26.843, 'eval_steps_per_second': 6.711, 'epoch': 12.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fc9053b6c04489193b6b8ba7ca52336",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.17112325131893158, 'eval_accuracy': 96.22641509433963, 'eval_runtime': 8.0524, 'eval_samples_per_second': 26.328, 'eval_steps_per_second': 6.582, 'epoch': 13.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2d84b06ec574351abe41c5d24192633",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1830337941646576, 'eval_accuracy': 95.28301886792453, 'eval_runtime': 8.194, 'eval_samples_per_second': 25.873, 'eval_steps_per_second': 6.468, 'epoch': 14.0}\n",
      "{'loss': 0.011, 'grad_norm': 0.005021353252232075, 'learning_rate': 0.0001308411214953271, 'epoch': 14.02}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18975fe6c1204c30a4b0a3d90eb9f5a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.18218187987804413, 'eval_accuracy': 94.81132075471697, 'eval_runtime': 8.0089, 'eval_samples_per_second': 26.47, 'eval_steps_per_second': 6.618, 'epoch': 15.0}\n",
      "{'train_runtime': 1236.0515, 'train_samples_per_second': 10.352, 'train_steps_per_second': 2.597, 'train_loss': 0.44819680712304755, 'epoch': 15.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3210, training_loss=0.44819680712304755, metrics={'train_runtime': 1236.0515, 'train_samples_per_second': 10.352, 'train_steps_per_second': 2.597, 'total_flos': 3460510521077760.0, 'train_loss': 0.44819680712304755, 'epoch': 15.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()*100}\n",
    "\n",
    "\n",
    "# The HuggingFace Trainer class handles the training and eval loop for PyTorch for us.\n",
    "# Read more about it here https://huggingface.co/docs/transformers/main_classes/trainer\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"bert-lora\",\n",
    "        learning_rate=2e-3,\n",
    "        # Reduce the batch size if you don't have enough memory\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        num_train_epochs=15,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "    ),\n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    eval_dataset=tokenized_ds[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"Starting to train...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save PEFT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Name: adapter_config.json; File Size: 0.68KB\n",
      "File Name: adapter_model.safetensors; File Size: 9288.92KB\n",
      "File Name: README.md; File Size: 4.97KB\n"
     ]
    }
   ],
   "source": [
    "peft_bert_model_path = \"fine-tuned-peft-model-weights/\"\n",
    "peft_model.save_pretrained(peft_bert_model_path)\n",
    "\n",
    "# check the size of the saved model\n",
    "for file_name in os.listdir(peft_bert_model_path):\n",
    "    file_size = os.path.getsize(peft_bert_model_path + file_name)\n",
    "    print(f\"File Name: {file_name}; File Size: {file_size / 1024:.2f}KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict using saved PEFT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3175a360a5cf442e808709f3d8b2d921",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PredictionOutput(predictions=array([[ -8.259553  ,  -6.122034  , -13.978308  , ...,   0.65173936,\n",
      "         -0.40767723,   4.227573  ],\n",
      "       [ -2.1437595 ,  -5.514235  ,  -8.889157  , ...,  -1.5703596 ,\n",
      "         -0.38314834,   3.6594245 ],\n",
      "       [ -3.5587769 ,  -8.240544  ,  -8.518811  , ...,   5.849249  ,\n",
      "         -3.198     ,  -5.308662  ],\n",
      "       ...,\n",
      "       [  6.2493873 ,  -6.7055526 ,  23.585546  , ...,   3.6332254 ,\n",
      "         10.616182  ,  -2.0770226 ],\n",
      "       [  3.6201847 ,  -8.221383  ,  20.452805  , ...,   3.485164  ,\n",
      "         11.051069  ,  -0.9255245 ],\n",
      "       [ -8.896778  ,  -4.8086658 ,  -6.2238364 , ...,   8.318126  ,\n",
      "         -2.9648018 ,  -4.5387554 ]], dtype=float32), label_ids=array([16, 16,  8, 17,  9,  3, 10, 18,  1, 17,  1,  3, 14, 19, 16, 14, 15,\n",
      "        2,  7, 13, 15, 14, 10, 14,  4,  1, 20,  4,  9, 13, 19,  3, 17,  2,\n",
      "        4, 10, 12, 11, 19,  9,  5, 11,  1, 12, 15,  8,  7, 17, 20,  6,  6,\n",
      "        4, 15,  3, 14, 13, 14,  5,  7, 17, 10,  2, 21,  5, 10, 18,  3, 16,\n",
      "        7,  7,  5,  5, 20, 13, 17,  4,  9, 19, 10, 15, 12,  6,  0,  1,  8,\n",
      "        0,  7, 12, 20,  0, 11,  5, 14,  4,  0,  1,  6,  4,  7, 21, 15, 11,\n",
      "       20, 10,  1,  0,  7,  3, 12, 10,  9,  6, 15,  5, 10, 18, 17,  3,  9,\n",
      "       16, 14,  0, 18, 11, 12,  4,  9, 12,  0, 17,  3, 18, 19,  8, 14,  7,\n",
      "       12, 21, 20, 19, 13, 20, 12, 18, 21,  5, 16, 11,  2, 11, 21, 20, 16,\n",
      "       14, 18, 15,  2, 20, 18,  6,  6, 11, 17,  0, 21, 21,  8, 21,  9,  3,\n",
      "       21, 19,  1,  9,  8, 16,  0, 10, 16,  3, 19, 11, 13, 12, 17,  2, 11,\n",
      "        5, 15, 19,  4,  5,  7, 15,  2,  6,  1,  8, 21,  0, 13, 18,  1,  8,\n",
      "        2, 16,  4,  6, 18,  2,  2,  6], dtype=int64), metrics={'test_loss': 0.156297504901886, 'test_model_preparation_time': 0.004, 'test_runtime': 7.8567, 'test_samples_per_second': 26.983, 'test_steps_per_second': 3.437})\n"
     ]
    }
   ],
   "source": [
    "# load PEFT model and predict \n",
    "\n",
    "config = PeftConfig.from_pretrained('fine-tuned-peft-model-weights/')\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", \n",
    "                                                    num_labels=22)\n",
    "model = PeftModel.from_pretrained(model, 'fine-tuned-peft-model-weights/')\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer)\n",
    ")\n",
    "\n",
    "test_predictions = trainer.predict(tokenized_ds['test'])\n",
    "print(test_predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
