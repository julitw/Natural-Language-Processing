{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertForTokenClassification, BertTokenizer,  BertForMaskedLM, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_recall_fscore_support, confusion_matrix\n",
    "import numpy as np\n",
    "import os\n",
    "from transformers import BertTokenizerFast\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julita/anaconda3/envs/myenv/lib/python3.10/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"wnut_17\", trust_remote_code = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@paulwalk',\n",
       " 'It',\n",
       " \"'s\",\n",
       " 'the',\n",
       " 'view',\n",
       " 'from',\n",
       " 'where',\n",
       " 'I',\n",
       " \"'m\",\n",
       " 'living',\n",
       " 'for',\n",
       " 'two',\n",
       " 'weeks',\n",
       " '.',\n",
       " 'Empire',\n",
       " 'State',\n",
       " 'Building',\n",
       " '=',\n",
       " 'ESB',\n",
       " '.',\n",
       " 'Pretty',\n",
       " 'bad',\n",
       " 'storm',\n",
       " 'here',\n",
       " 'last',\n",
       " 'evening',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 0,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]['ner_tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'B-corporation',\n",
       " 'I-corporation',\n",
       " 'B-creative-work',\n",
       " 'I-creative-work',\n",
       " 'B-group',\n",
       " 'I-group',\n",
       " 'B-location',\n",
       " 'I-location',\n",
       " 'B-person',\n",
       " 'I-person',\n",
       " 'B-product',\n",
       " 'I-product']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list = dataset[\"train\"].features[f\"ner_tags\"].feature.names\n",
    "num_labels = len(label_list)\n",
    "label_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>ner_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@paulwalk It 's the view from where I 'm livin...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From Green Newsfeed : AHFA extends deadline fo...</td>\n",
       "      <td>[0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pxleyes Top 50 Photography Contest Pictures of...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>today is my last day at the office .</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4Dbling 's place til monday , party party part...</td>\n",
       "      <td>[9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>watching the VMA pre-show again lol it was n't...</td>\n",
       "      <td>[0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>27 followers ! 30 followers is my goal for tod...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This is the 2nd hospital ive been in today , b...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Friday Night Eats http://twitpic.com/2pdvtr</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Gotta dress up for london fashion week and par...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tokens  \\\n",
       "0  @paulwalk It 's the view from where I 'm livin...   \n",
       "1  From Green Newsfeed : AHFA extends deadline fo...   \n",
       "2  Pxleyes Top 50 Photography Contest Pictures of...   \n",
       "3               today is my last day at the office .   \n",
       "4  4Dbling 's place til monday , party party part...   \n",
       "5  watching the VMA pre-show again lol it was n't...   \n",
       "6  27 followers ! 30 followers is my goal for tod...   \n",
       "7  This is the 2nd hospital ive been in today , b...   \n",
       "8        Friday Night Eats http://twitpic.com/2pdvtr   \n",
       "9  Gotta dress up for london fashion week and par...   \n",
       "\n",
       "                                            ner_tags  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, ...  \n",
       "1      [0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "2               [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "3                        [0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "4               [9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "5  [0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "6                  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "7  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "8                                       [0, 0, 0, 0]  \n",
       "9               [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data = dataset[\"train\"].select(range(10)) \n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"tokens\": [\" \".join(example[\"tokens\"]) for example in sample_data],\n",
    "    \"ner_tags\": [example[\"ner_tags\"] for example in sample_data]\n",
    "})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B- indicates the beginning of an entity.\n",
    "I- indicates a token is contained inside the same entity (for example, the State token is a part of an entity like Empire State Building).\n",
    "0 indicates the token doesnâ€™t correspond to any entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example = dataset[\"train\"][0]\n",
    "# tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
    "# tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "# tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3394/3394 [00:00<00:00, 16280.79 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_16701/3085954879.py:89: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define a function to tokenize and align the labels\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        padding='max_length',\n",
    "        max_length=128\n",
    "    )\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples['ner_tags']):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        label_ids = []\n",
    "        previous_word_idx = None\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Tokenize dataset and align labels\n",
    "tokenized_datasets = dataset.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "# Load the model for token classification\n",
    "model = BertForTokenClassification.from_pretrained(\"bert-base-cased\", num_labels=num_labels)\n",
    "\n",
    "# Load the accuracy  and f1 metric\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "# Define the compute_metrics function\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [p for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [l for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    # Convert predictions and labels from label indices to integers\n",
    "    true_predictions_flat = [int(item) for sublist in true_predictions for item in sublist]\n",
    "    true_labels_flat = [int(item) for sublist in true_labels for item in sublist]\n",
    "\n",
    "    incorrect_examples = []\n",
    "    for idx, (pred, label) in enumerate(zip(true_predictions, true_labels)):\n",
    "        if pred != label:\n",
    "            tokens = tokenized_datasets[\"validation\"][\"tokens\"][idx]  # Original tokens for the example\n",
    "            incorrect_examples.append({\n",
    "                \"tokens\": tokens,\n",
    "                \"true_labels\": label,\n",
    "                \"predicted_labels\": pred\n",
    "            })\n",
    "\n",
    "    # Save incorrect examples to a CSV file if any errors are present\n",
    "    if incorrect_examples:\n",
    "        df_incorrect = pd.DataFrame(incorrect_examples)\n",
    "        df_incorrect.to_csv(\"incorrect_predictions_trained.csv\", index=False)\n",
    "\n",
    "    accuracy = metric.compute(predictions=true_predictions_flat, references=true_labels_flat)\n",
    "    f1 = f1_metric.compute(predictions=true_predictions_flat, references=true_labels_flat, average=\"weighted\")[\"f1\"]\n",
    "\n",
    "    return {\"accuracy\": accuracy, \"f1\": f1}\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'tokens', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 3394\n",
       "})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [01:03<00:00,  1.99it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.6555144786834717,\n",
       " 'eval_model_preparation_time': 0.0015,\n",
       " 'eval_accuracy': {'accuracy': 0.020275853301976735},\n",
       " 'eval_f1': 0.02978602208907957,\n",
       " 'eval_runtime': 64.2228,\n",
       " 'eval_samples_per_second': 15.711,\n",
       " 'eval_steps_per_second': 1.977}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 425/1275 [09:58<16:04,  1.14s/it]\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 425/1275 [10:50<16:04,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.22069907188415527, 'eval_model_preparation_time': 0.0015, 'eval_accuracy': {'accuracy': 0.9497235110913367}, 'eval_f1': 0.9369062659629562, 'eval_runtime': 51.7912, 'eval_samples_per_second': 19.482, 'eval_steps_per_second': 2.452, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|â–ˆâ–ˆâ–ˆâ–‰      | 500/1275 [12:35<17:52,  1.38s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1921, 'grad_norm': 3.9897754192352295, 'learning_rate': 1.215686274509804e-05, 'epoch': 1.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 850/1275 [20:13<07:44,  1.09s/it]\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 850/1275 [21:04<07:44,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.22599846124649048, 'eval_model_preparation_time': 0.0015, 'eval_accuracy': {'accuracy': 0.9529015445242484}, 'eval_f1': 0.9445694066365404, 'eval_runtime': 50.8077, 'eval_samples_per_second': 19.859, 'eval_steps_per_second': 2.5, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1000/1275 [24:32<06:14,  1.36s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0676, 'grad_norm': 2.1176671981811523, 'learning_rate': 4.313725490196079e-06, 'epoch': 2.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1275/1275 [30:52<00:00,  1.12s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1275/1275 [31:45<00:00,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.24843436479568481, 'eval_model_preparation_time': 0.0015, 'eval_accuracy': {'accuracy': 0.954427000572046}, 'eval_f1': 0.9474851760387836, 'eval_runtime': 52.0289, 'eval_samples_per_second': 19.393, 'eval_steps_per_second': 2.441, 'epoch': 3.0}\n",
      "{'train_runtime': 1905.083, 'train_samples_per_second': 5.345, 'train_steps_per_second': 0.669, 'train_loss': 0.11149864720363242, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1275, training_loss=0.11149864720363242, metrics={'train_runtime': 1905.083, 'train_samples_per_second': 5.345, 'train_steps_per_second': 0.669, 'total_flos': 665197041756672.0, 'train_loss': 0.11149864720363242, 'epoch': 3.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julita/anaconda3/envs/myenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM, DataCollatorForLanguageModeling\n",
    "import random\n",
    "import torch\n",
    "from datasets import DatasetDict\n",
    "\n",
    "# Load the tokenizer and Masked Language Model\n",
    "tokenizer_mlm = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "mlm_model = BertForMaskedLM.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# Define a data collator for language modeling to automatically mask tokens\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer_mlm,\n",
    "    mlm=True,\n",
    "    mlm_probability=0.15\n",
    ")\n",
    "\n",
    "# Define a function to perform token masking and augmentation using MLM\n",
    "def augment_data_with_mlm(examples):\n",
    "    # Tokenize input tokens\n",
    "    inputs = tokenizer_mlm(\n",
    "        examples[\"tokens\"],\n",
    "        is_split_into_words=True,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # Prepare inputs for data collator\n",
    "    batch_for_collator = [{\"input_ids\": input_ids} for input_ids in inputs[\"input_ids\"]]\n",
    "    \n",
    "    # Mask tokens using the data collator\n",
    "    collated_batch = data_collator(batch_for_collator)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get MLM predictions for masked tokens\n",
    "        outputs = mlm_model(**collated_batch)\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "    # Replace masked tokens with predictions in input tokens\n",
    "    augmented_tokens = []\n",
    "    for idx, token_ids in enumerate(collated_batch[\"input_ids\"]):\n",
    "        tokens = tokenizer_mlm.convert_ids_to_tokens(token_ids)\n",
    "        predicted_tokens = tokenizer_mlm.convert_ids_to_tokens(predictions[idx])\n",
    "        augmented_example = [\n",
    "            predicted_tokens[i] if token == tokenizer_mlm.mask_token else token\n",
    "            for i, token in enumerate(tokens)\n",
    "        ]\n",
    "        augmented_tokens.append(augmented_example)\n",
    "\n",
    "    # Reformat the augmented data to align with the original dataset structure\n",
    "    augmented_examples = {\"tokens\": augmented_tokens, \"ner_tags\": examples[\"ner_tags\"]}\n",
    "    return augmented_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "# Apply augmentation to the training dataset\n",
    "augmented_dataset = dataset[\"train\"].map(augment_data_with_mlm, batched=True)\n",
    "\n",
    "# Combine original and augmented data using concatenate_datasets\n",
    "train_dataset_combined = concatenate_datasets([dataset[\"train\"], augmented_dataset])\n",
    "\n",
    "# Wrap it back into a DatasetDict for compatibility with Trainer\n",
    "combined_datasets = DatasetDict({\"train\": train_dataset_combined, \"validation\": dataset[\"validation\"]})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-cased\")\n",
    "model = BertForTokenClassification.from_pretrained(\"bert-base-cased\", num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to tokenize and align the labels\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        padding='max_length',\n",
    "        max_length=128\n",
    "    )\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples['ner_tags']):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        label_ids = []\n",
    "        previous_word_idx = None\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                # Ensure word_idx is within bounds of the label list\n",
    "                if word_idx < len(label):\n",
    "                    label_ids.append(label[word_idx])\n",
    "                else:\n",
    "                    label_ids.append(-100)\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16701/1019695210.py:15: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_augmented = Trainer(\n",
      " 20%|â–ˆâ–‰        | 500/2547 [11:01<45:56,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.268, 'grad_norm': 2.4735350608825684, 'learning_rate': 1.607381232822929e-05, 'epoch': 0.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 849/2547 [20:12<33:17,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2131391167640686, 'eval_accuracy': {'accuracy': 0.9525837411809572}, 'eval_f1': 0.9421620495851264, 'eval_runtime': 53.2406, 'eval_samples_per_second': 18.952, 'eval_steps_per_second': 2.385, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1000/2547 [23:40<34:42,  1.35s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1778, 'grad_norm': 1.8735758066177368, 'learning_rate': 1.2147624656458579e-05, 'epoch': 1.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1500/2547 [35:37<23:36,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1464, 'grad_norm': 4.771763801574707, 'learning_rate': 8.221436984687869e-06, 'epoch': 1.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1698/2547 [41:04<16:56,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2352842390537262, 'eval_accuracy': {'accuracy': 0.953600711879489}, 'eval_f1': 0.9460097377846362, 'eval_runtime': 50.1798, 'eval_samples_per_second': 20.108, 'eval_steps_per_second': 2.531, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2000/2547 [47:51<12:26,  1.36s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1212, 'grad_norm': 1.446900486946106, 'learning_rate': 4.295249312917158e-06, 'epoch': 2.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2500/2547 [59:22<01:03,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1062, 'grad_norm': 2.0176374912261963, 'learning_rate': 3.6906164114644683e-07, 'epoch': 2.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2547/2547 [1:01:17<00:00,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.24378447234630585, 'eval_accuracy': {'accuracy': 0.9548083645839954}, 'eval_f1': 0.9485204388055916, 'eval_runtime': 50.5788, 'eval_samples_per_second': 19.949, 'eval_steps_per_second': 2.511, 'epoch': 3.0}\n",
      "{'train_runtime': 3677.8938, 'train_samples_per_second': 5.537, 'train_steps_per_second': 0.693, 'train_loss': 0.16278490535315224, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2547, training_loss=0.16278490535315224, metrics={'train_runtime': 3677.8938, 'train_samples_per_second': 5.537, 'train_steps_per_second': 0.693, 'total_flos': 1330394083513344.0, 'train_loss': 0.16278490535315224, 'epoch': 3.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the combined dataset for NER training\n",
    "tokenized_combined_datasets = combined_datasets.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Reinitialize the Trainer with the augmented dataset\n",
    "trainer_augmented = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_combined_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_combined_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train the NER model on augmented data\n",
    "trainer_augmented.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
