{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# Check if CUDA is available\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"Using CUDA device:\", device)\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"CUDA not available, using CPU device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZMkOFyRjYjS",
        "outputId": "c0264be8-d500-4c90-cf5f-3d309902a5f9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using CUDA device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lUctI8KzBW6W"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.nn import functional as F\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel\n",
        "from tabulate import tabulate\n",
        "from tqdm import tqdm, trange\n",
        "from copy import deepcopy\n",
        "import numpy as np\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ALNUM_CHARSET = set('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789')\n",
        "\n",
        "def convert_to_tokens(indices, tokenizer, extended=False, extra_values_pos=None, strip=True):\n",
        "    if extended:\n",
        "        res = [tokenizer.convert_ids_to_tokens([idx])[0] if idx < len(tokenizer) else\n",
        "               (f\"[pos{idx-len(tokenizer)}]\" if idx < extra_values_pos else f\"[val{idx-extra_values_pos}]\")\n",
        "               for idx in indices]\n",
        "    else:\n",
        "        res = tokenizer.convert_ids_to_tokens(indices)\n",
        "    if strip:\n",
        "        res = list(map(lambda x: x[1:] if x[0] == 'Ġ' else \"#\" + x, res))\n",
        "    return res\n",
        "\n",
        "\n",
        "def top_tokens(v, k=100, tokenizer=None, only_alnum=False, only_ascii=True, with_values=False,\n",
        "               exclude_brackets=False, extended=True, extra_values=None, only_from_list=None):\n",
        "    if tokenizer is None:\n",
        "        tokenizer = my_tokenizer\n",
        "    v = deepcopy(v)\n",
        "    ignored_indices = []\n",
        "    if only_ascii:\n",
        "        ignored_indices.extend([key for val, key in tokenizer.vocab.items() if not val.strip('Ġ▁').isascii()])\n",
        "    if only_alnum:\n",
        "        ignored_indices.extend([key for val, key in tokenizer.vocab.items() if not (set(val.strip('Ġ▁[] ')) <= ALNUM_CHARSET)])\n",
        "    if only_from_list:\n",
        "        ignored_indices.extend([key for val, key in tokenizer.vocab.items() if val.strip('Ġ▁ ').lower() not in only_from_list])\n",
        "    if exclude_brackets:\n",
        "        ignored_indices = set(ignored_indices).intersection(\n",
        "            {key for val, key in tokenizer.vocab.items() if not (val.isascii() and val.isalnum())})\n",
        "        ignored_indices = list(ignored_indices)\n",
        "\n",
        "    ignored_indices = list(set(ignored_indices))\n",
        "    v[ignored_indices] = -np.inf\n",
        "    extra_values_pos = len(v)\n",
        "    if extra_values is not None:\n",
        "        v = torch.cat([v, extra_values])\n",
        "    values, indices = torch.topk(v, k=k)\n",
        "    res = convert_to_tokens(indices, tokenizer, extended=extended, extra_values_pos=extra_values_pos)\n",
        "    if with_values:\n",
        "        res = list(zip(res, values.cpu().numpy()))\n",
        "    return res"
      ],
      "metadata": {
        "id": "rMooKWkKBbTy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def print_top_tokens(i1, i2, K_heads, V_heads, emb, tokens_list=None):\n",
        "\n",
        "    print(f'Layer {i1}, Neuron {i2}')\n",
        "    print(\n",
        "        tabulate(\n",
        "            [\n",
        "                *zip(\n",
        "                    top_tokens(\n",
        "                        (K_heads[i1, i2]) @ emb,\n",
        "                        k=30,\n",
        "                        only_from_list=tokens_list,\n",
        "                        only_alnum=False,\n",
        "                    ),\n",
        "                    top_tokens(\n",
        "                        (V_heads[i1, i2]) @ emb,\n",
        "                        k=30,\n",
        "                        only_from_list=tokens_list,\n",
        "                        only_alnum=False,\n",
        "                    ),\n",
        "                    top_tokens((-K_heads[i1, i2]) @ emb, k=30, only_from_list=tokens_list),\n",
        "                    top_tokens((-V_heads[i1, i2]) @ emb, k=30, only_from_list=tokens_list),\n",
        "                )\n",
        "            ],\n",
        "            headers=[\"K\", \"V\", \"-K\", \"-V\"],\n",
        "        )\n",
        "    )"
      ],
      "metadata": {
        "id": "mizdn5buG-iC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exclude_same = False\n",
        "reverse_list = False\n",
        "only_ascii = True\n",
        "only_alnum = False\n",
        "\n",
        "def approx_topk(mat, min_k=500, max_k=50_000, th0=10, max_iters=10, verbose=False):\n",
        "    def _get_actual_k(th, th_max):\n",
        "        # Split the computation into chunks to reduce memory usage\n",
        "        chunk_size = 1024  # Adjust this value based on your GPU memory\n",
        "        num_chunks = (mat.shape[0] + chunk_size - 1) // chunk_size\n",
        "        actual_k = 0\n",
        "        for i in range(num_chunks):\n",
        "            start = i * chunk_size\n",
        "            end = min(start + chunk_size, mat.shape[0])\n",
        "            chunk = mat[start:end]\n",
        "            actual_k += torch.nonzero((chunk > th) & (chunk < th_max)).shape[0]\n",
        "        return actual_k\n",
        "\n",
        "    th_max = np.inf\n",
        "    left, right = 0, th0\n",
        "    while True:\n",
        "        actual_k = _get_actual_k(right, th_max)\n",
        "        if verbose:\n",
        "            print(f\"one more iteration. {actual_k}\")\n",
        "        if actual_k <= max_k:\n",
        "            break\n",
        "        left, right = right, right * 2\n",
        "    if min_k <= actual_k <= max_k:\n",
        "        th = right\n",
        "    else:\n",
        "        for _ in range(max_iters):\n",
        "            mid = (left + right) / 2\n",
        "            actual_k = _get_actual_k(mid, th_max)\n",
        "            if verbose:\n",
        "                print(f\"one more iteration. {actual_k}\")\n",
        "            if min_k <= actual_k <= max_k:\n",
        "                break\n",
        "            if actual_k > max_k:\n",
        "                left = mid\n",
        "            else:\n",
        "                right = mid\n",
        "        th = mid\n",
        "\n",
        "    # Get the nonzero indices in chunks\n",
        "    all_indices = []\n",
        "    chunk_size = 1024  # Adjust this value based on your GPU memory\n",
        "    num_chunks = (mat.shape[0] + chunk_size - 1) // chunk_size\n",
        "    for i in range(num_chunks):\n",
        "        start = i * chunk_size\n",
        "        end = min(start + chunk_size, mat.shape[0])\n",
        "        chunk = mat[start:end]\n",
        "        indices = torch.nonzero((chunk > th) & (chunk < th_max))\n",
        "        # Adjust indices to reflect the original matrix\n",
        "        indices[:, 0] += start\n",
        "        all_indices.extend(indices.tolist())\n",
        "\n",
        "    return all_indices\n",
        "def get_top_entries(tmp, all_high_pos, only_ascii=False, only_alnum=False, exclude_same=False, exclude_fuzzy=False, tokens_list=None):\n",
        "    remaining_pos = all_high_pos\n",
        "    if only_ascii:\n",
        "        remaining_pos = [*filter(\n",
        "            lambda x: (tokenizer.decode(x[0]).strip('Ġ▁').isascii() and tokenizer.decode(x[1]).strip('Ġ▁').isascii()),\n",
        "            remaining_pos)]\n",
        "    if only_alnum:\n",
        "        remaining_pos = [*filter(\n",
        "            lambda x: (tokenizer.decode(x[0]).strip('Ġ▁ ').isalnum() and tokenizer.decode(x[1]).strip('Ġ▁ ').isalnum()),\n",
        "            remaining_pos)]\n",
        "    if exclude_same:\n",
        "        remaining_pos = [*filter(\n",
        "            lambda x: tokenizer.decode(x[0]).lower().strip() != tokenizer.decode(x[1]).lower().strip(),\n",
        "            remaining_pos)]\n",
        "    if exclude_fuzzy:\n",
        "        remaining_pos = [*filter(\n",
        "            lambda x: not _fuzzy_eq(tokenizer.decode(x[0]).lower().strip(), tokenizer.decode(x[1]).lower().strip()),\n",
        "            remaining_pos)]\n",
        "    if tokens_list:\n",
        "        remaining_pos = [*filter(\n",
        "            lambda x: ((tokenizer.decode(x[0]).strip('Ġ▁').lower().strip() in tokens_list) and\n",
        "                       (tokenizer.decode(x[1]).strip('Ġ▁').lower().strip() in tokens_list)),\n",
        "            remaining_pos)]\n",
        "\n",
        "    pos_val = tmp[[*zip(*remaining_pos)]]\n",
        "    good_cells = [*map(lambda x: (tokenizer.decode(x[0]), tokenizer.decode(x[1])), remaining_pos)]\n",
        "    good_tokens = list(map(lambda x: Counter(x).most_common(), zip(*good_cells)))\n",
        "    remaining_pos_best = np.array(remaining_pos)[torch.argsort(pos_val if reverse_list else -pos_val).cpu()[:50]]\n",
        "    good_cells_best = [*map(lambda x: (tokenizer.decode(x[0]), tokenizer.decode(x[1])), remaining_pos_best)]\n",
        "    # good_cells[:100]\n",
        "    # list(zip(good_tokens[0], good_tokens[1]))\n",
        "    return good_cells_best"
      ],
      "metadata": {
        "id": "sdLgdQSGHKgT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#GPT2"
      ],
      "metadata": {
        "id": "loyM1Q12tmow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\"sdadas/polish-gpt2-medium\")\n",
        "tokenizer = my_tokenizer = AutoTokenizer.from_pretrained(\"sdadas/polish-gpt2-medium\")\n",
        "emb = model.get_output_embeddings().weight.data.T.detach()\n",
        "emb = model.get_output_embeddings().weight.data.T.detach()\n",
        "model.config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93VGQTjPBgN1",
        "outputId": "09970e97-962f-4edd-836e-7d97e08d5a8e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2Config {\n",
              "  \"_attn_implementation_autoset\": true,\n",
              "  \"_name_or_path\": \"sdadas/polish-gpt2-medium\",\n",
              "  \"activation_function\": \"gelu_fast\",\n",
              "  \"architectures\": [\n",
              "    \"GPT2LMHeadModel\"\n",
              "  ],\n",
              "  \"attn_pdrop\": 0.1,\n",
              "  \"bos_token_id\": 0,\n",
              "  \"embd_pdrop\": 0.1,\n",
              "  \"eos_token_id\": 2,\n",
              "  \"initializer_range\": 0.02,\n",
              "  \"layer_norm_epsilon\": 1e-05,\n",
              "  \"model_type\": \"gpt2\",\n",
              "  \"n_embd\": 1024,\n",
              "  \"n_head\": 16,\n",
              "  \"n_inner\": 4096,\n",
              "  \"n_layer\": 24,\n",
              "  \"n_positions\": 2048,\n",
              "  \"reorder_and_upcast_attn\": false,\n",
              "  \"resid_pdrop\": 0.1,\n",
              "  \"scale_attn_by_inverse_layer_idx\": false,\n",
              "  \"scale_attn_weights\": true,\n",
              "  \"summary_activation\": null,\n",
              "  \"summary_first_dropout\": 0.1,\n",
              "  \"summary_proj_to_labels\": true,\n",
              "  \"summary_type\": \"cls_index\",\n",
              "  \"summary_use_proj\": true,\n",
              "  \"tokenizer_class\": \"GPT2TokenizerFast\",\n",
              "  \"torch_dtype\": \"float32\",\n",
              "  \"transformers_version\": \"4.46.2\",\n",
              "  \"use_cache\": true,\n",
              "  \"vocab_size\": 51200\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ekstrakcja wag z modelu GPT2"
      ],
      "metadata": {
        "id": "g--xlmejt3wl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "num_layers = model.config.n_layer\n",
        "num_heads = model.config.n_head\n",
        "hidden_dim = model.config.n_embd\n",
        "head_size = hidden_dim // num_heads\n",
        "\n",
        "K = torch.cat([model.get_parameter(f\"transformer.h.{j}.mlp.c_fc.weight\").T\n",
        "                           for j in range(num_layers)]).detach()\n",
        "V = torch.cat([model.get_parameter(f\"transformer.h.{j}.mlp.c_proj.weight\")\n",
        "                           for j in range(num_layers)]).detach()\n",
        "\n",
        "W_Q, W_K, W_V = torch.cat([model.get_parameter(f\"transformer.h.{j}.attn.c_attn.weight\")\n",
        "                           for j in range(num_layers)]).detach().chunk(3, dim=-1)\n",
        "W_O = torch.cat([model.get_parameter(f\"transformer.h.{j}.attn.c_proj.weight\")\n",
        "                           for j in range(num_layers)]).detach()\n",
        "\n",
        "K_heads = K.reshape(num_layers, -1, hidden_dim)\n",
        "V_heads = V.reshape(num_layers, -1, hidden_dim)\n",
        "d_int = K_heads.shape[1]\n",
        "\n",
        "W_Q_heads = W_Q.reshape(num_layers, hidden_dim, num_heads, head_size).permute(0, 2, 1, 3)\n",
        "W_K_heads = W_K.reshape(num_layers, hidden_dim, num_heads, head_size).permute(0, 2, 1, 3)\n",
        "W_V_heads = W_V.reshape(num_layers, hidden_dim, num_heads, head_size).permute(0, 2, 1, 3)\n",
        "W_O_heads = W_O.reshape(num_layers, num_heads, head_size, hidden_dim)\n",
        "emb_inv = emb.T\n",
        "print(emb_inv.shape)\n",
        "print(f\"Layers in the model: {num_layers}\")\n",
        "print(f\"Neurons in the model: {model.config.n_inner}\")"
      ],
      "metadata": {
        "id": "Dn57DT6zKz6V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e574fbb-d394-4f8e-968c-3de82b30b647"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([51200, 1024])\n",
            "Layers in the model: 24\n",
            "Neurons in the model: 4096\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(K_heads.shape)\n",
        "print(V_heads.shape)\n",
        "print(W_Q_heads.shape)\n",
        "print(W_K_heads.shape)\n",
        "print(W_V_heads.shape)\n",
        "print(W_O_heads.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhvWtbFSjPio",
        "outputId": "11a64b2f-6841-4dc9-c58f-f42951239338"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([24, 4096, 1024])\n",
            "torch.Size([24, 4096, 1024])\n",
            "torch.Size([24, 16, 1024, 64])\n",
            "torch.Size([24, 16, 1024, 64])\n",
            "torch.Size([24, 16, 1024, 64])\n",
            "torch.Size([24, 16, 64, 1024])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interpretacja wag modelu GPT2 na pustej liście tokenów"
      ],
      "metadata": {
        "id": "J3GDF7rxt9Hf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_list = set()"
      ],
      "metadata": {
        "id": "xX7P-XSVCxYP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Przykład z notebook'a\n",
        "ilayer = 23\n",
        "ineuron = 907\n",
        "print_top_tokens(ilayer, ineuron, K_heads, V_heads, emb, tokens_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-IFh-PruM6td",
        "outputId": "06bed1da-8088-4fcb-c0cd-1ebd0157dc4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer 23, Neuron 907\n",
            "K          V              -K          -V\n",
            "---------  -------------  ----------  ---------\n",
            "przycu     kody           dotychczas  #bot\n",
            "zalog      #gory          rodzi       #lot\n",
            "wylegi     #ei            do          #dzista\n",
            "#walifik   #zmy           przebie     #remont\n",
            "przesp     Apo            gatunku     #lee\n",
            "pochowany  apokali        przez       #wan\n",
            "#ppe       #128           #ja         #ette\n",
            "wep        ludy           rodzin      #up\n",
            "sfinans    #ords          zrazu       #bul\n",
            "#iss       Cezary         lokalnie    #puszczam\n",
            "#CS        archa          porywa      spu\n",
            "#gny       przy           two         zamyka\n",
            "#zwol      Homo           jeszcze     odstawi\n",
            "#-).       litera         #jaw        #mont\n",
            "lock       #pka           na          #beki\n",
            "skonfisk   Benedykt       wymaga      #laks\n",
            "#post      narodem        ty          tap\n",
            "postoju    polityki       Drze        poby\n",
            "erek       symbolu        pod         posto\n",
            "#ionu      #lachet        Nie         #wana\n",
            "#ppo       publicznego    rzuca       #dzana\n",
            "unierucho  #rzami         ludzkim     #load\n",
            "#ott       anie           uczucia     #owol\n",
            "zbombar    uniwersalny    rze         zatk\n",
            "#FF        artystycznego  tak         opuszcza\n",
            "#prem      ustawowego     da          znajd\n",
            "#klaski    werb           ludzkie     #dzany\n",
            "#gs        wzroku         natury      #elli\n",
            "zatk       #chnienia      #obie       #bek\n",
            "#hony      #osc           pewnego     zrealiz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "nauka"
      ],
      "metadata": {
        "id": "rS_4w4QYNj4y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ostatni neuron w ostatniej warstwie\n",
        "ilayer = 23\n",
        "ineuron = 4034\n",
        "print_top_tokens(ilayer, ineuron, K_heads, V_heads, emb, tokens_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5Ut1GGvM86v",
        "outputId": "ff987626-ab77-4f1d-c148-80893dbadbf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer 23, Neuron 4034\n",
            "K             V             -K           -V\n",
            "------------  ------------  -----------  ---------\n",
            "matematyka    #fur          #rost        #znania\n",
            "#dysk         #ki           #ariu        #rda\n",
            "#pedia        Timo          #obro        instancji\n",
            "lektura       #kowe         #sina        #zowie\n",
            "przysm        #kowych       usta         #ida\n",
            "#najmniej     #kov          #lat         #racja\n",
            "lektury       obejmuje      #kal         #rzo\n",
            "nauka         #gary         #hala        #rzysty\n",
            "naukowy       #Zwie         wzniesi      #rzono\n",
            "#resort       Bran          #klat        #jasz\n",
            "#BN           Brand         #ari         #rab\n",
            "pamie         #+            linii        #datek\n",
            "najwybit      #hau          #atem        #tta\n",
            "najciekaw     #ce           Rami         #rada\n",
            "doktorat      #karzem       nieruchom    #rado\n",
            "najciekawsze  #Kom          #atu         #alog\n",
            "zjada         #Karo         #nio         #pierdol\n",
            "#owaj         Christian     Go           #ira\n",
            "naukowa       #com          Od           #dzo\n",
            "#wnik         #Wi           brzegach     #nda\n",
            "najpopular    #owatych      Fa           #urat\n",
            "WHO           umy           ery          #yd\n",
            "ulubi         Hau           bezsi        #mien\n",
            "encyklope     #kowo         #21          Dama\n",
            "#cyklope      miejsce       #lecia       #sina\n",
            "naukowe       #Fin          intensyw     #imie\n",
            "#fr           gdziekolwiek  niebieskich  #dnych\n",
            "populary      PW            ska          #dnego\n",
            "ekspertem     zamiast       brwi         #dnej\n",
            "dostep        #rysty        przekrzy     #szenko\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i1, i2 = 0, 0\n",
        "print_top_tokens(i1, i2, K_heads, V_heads, emb, tokens_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDuD0NdZNsbL",
        "outputId": "b6f7009b-2f82-415c-9949-980310695292"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer 0, Neuron 0\n",
            "K        V            -K             -V\n",
            "-------  -----------  -------------  ---------\n",
            "#uel     #pus         Tod            #plek\n",
            "#ul      #ont         #skom          #strzy\n",
            "#pul     #onek        podzie         #kli\n",
            "#gul     #sud         #dzie          #wat\n",
            "#ann     #onych       #sce           #liwie\n",
            "bel      #TK          #wie           #rgi\n",
            "#jal     #STWO        #max           uszy\n",
            "#cht     #oper        #stycznych     #RI\n",
            "#oll     #oty         rezerwat       #las\n",
            "#El      #BW          predy          #gre\n",
            "#ulf     #ARS         #system        #dle\n",
            "El       #poty        niespodzianki  #nisz\n",
            "el       #ones        niespodzianka  #genera\n",
            "dywiden  postoju      #styczne       #inga\n",
            "#alu     #sbur        #kaza          rozdz\n",
            "#atak    konspiracji  #rom           usposobie\n",
            "#itt     #ono         ciekawostki    #rwi\n",
            "#chol    #pes         #rick          #rodzi\n",
            "#anga    ognisk       #dac           #wska\n",
            "#al      #akami       przygod        #isa\n",
            "#eman    #ematy       #zna           naba\n",
            "#olony   #orus        #szki          #szny\n",
            "#aret    #szczyzna    fotografi      #rk\n",
            "#EL      #pet         #zka           #nota\n",
            "#CJ      Algier       #jazdu         #nastu\n",
            "#olu     #ologia      #zgi           #szkach\n",
            "Cul      #lanki       listy          zakrzy\n",
            "#ulu     krytyki      #szcze         #rr\n",
            "#amp     #ote         #dziemy        Zen\n",
            "#UL      plakat       #ric           leka\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Afganistan"
      ],
      "metadata": {
        "id": "5xaanHSqOHjS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ilayer = 5\n",
        "ineuron = 2031\n",
        "print_top_tokens(ilayer, ineuron, K_heads, V_heads, emb, tokens_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rn6BTLTAN3WX",
        "outputId": "5f62050f-0c4f-46a8-ae9f-0b2b6dc3ca66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer 5, Neuron 2031\n",
            "K               V          -K           -V\n",
            "--------------  ---------  -----------  -----------\n",
            "Afganistanu     Rzesz      #reb         emerytalne\n",
            "Afga            #parcie    fair         publiczne\n",
            "Jehowy          #tyz       #gle         #szto\n",
            "Wojny           #stie      #cie         ui\n",
            "Obywatelskich   #tz        #bal         #smo\n",
            "zbiorowych      #kop       #patrz       Sub\n",
            "etni            #tkami     #arter       #szard\n",
            "getta           #zdro      #gl          publiczna\n",
            "rdzenia         #lock      moimi        odprawy\n",
            "Kinga           #tto       mym          powsze\n",
            "etnicznych      #TS        #bki         #Ur\n",
            "Afganistanie    #wcze      moim         autorskie\n",
            "#zacje          #ieu       #pi          Anne\n",
            "przegranej      uderzeniu  #de          #mion\n",
            "emerytalnych    rzesz      #lar         integra\n",
            "wojennych       #sci       pas          ur\n",
            "Krajowych       #zej       biurko       publicznych\n",
            "zbiorowe        #zno       #dgo         celne\n",
            "uznanych        hamowania  #dalej       #wymiar\n",
            "mieszkaniowych  #stres     #czu         zawodowo\n",
            "Polsko          #oro       #step        bary\n",
            "nieulecz        Lucas      #skaki       publiczny\n",
            "obywat          szta       #trzym       #owicie\n",
            "podatkowa       #sterze    #dzwo        #Jon\n",
            "granicznych     kalenda    znajdziecie  var\n",
            "wojny           #tek       #rodziej     fel\n",
            "Miriam          #chor      #bek         #rekty\n",
            "afga            diecezji   spostrze     #nom\n",
            "#fikacji        #tora      #ather       Krajowy\n",
            "ubogich         oparciu    #gla         uma\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ustroje, rządy"
      ],
      "metadata": {
        "id": "z8DJ6jUsParA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ilayer = 11\n",
        "ineuron = 2076\n",
        "print_top_tokens(ilayer, ineuron, K_heads, V_heads, emb, tokens_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8PLMe7mOJv1",
        "outputId": "d10a509a-e593-45db-83fc-c8abaaf6af91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer 11, Neuron 2076\n",
            "K          V              -K          -V\n",
            "---------  -------------  ----------  ------\n",
            "Republi    #rowicza       #p          #zyn\n",
            "monarch    #HO            #wodni      #Miesz\n",
            "krajami    #hra           #alnie      #stin\n",
            "Republika  #bela          #mieni      #ZY\n",
            "Gut        generalnej     #alni       #spon\n",
            "monarchii  #kacz          #program    Spi\n",
            "ostra      Mazow          #lacyjny    #ntem\n",
            "jurysdy    wilki          przy        skopi\n",
            "poprzek    #szewskiego    #praw       #pet\n",
            "manifest   #gha           #niczki     #logi\n",
            "#rump      #ucha          #pla        #skor\n",
            "#omato     #anowskiego    #Ph         #sne\n",
            "zatar      Mazu           Ph          #imi\n",
            "#litary    bliski         #cika       staran\n",
            "zaciera    #gh            #ha         #LT\n",
            "kanclerz   #kiewicza      #szlo       pobra\n",
            "wete       #dztwa         #gr         pigu\n",
            "republika  #cha           #Zdro       #fr\n",
            "#omas      #chta          #mistrza    #jemu\n",
            "Douglas    #szno          #dzki       #Syl\n",
            "republi    #rwale         specjalnym  #stur\n",
            "kierowali  #wicza         #maj        Syn\n",
            "#dine      nowotwor       #beli       #Studi\n",
            "#schen     #PSL           #mistrz     #tj\n",
            "#banu      reki           #mara       #stina\n",
            "Museum     #chia          #ball       #styn\n",
            "Nort       niema          #lki        #faty\n",
            "Bawar      Moskwy         #dnie       #typ\n",
            "#leum      mazowieckiego  #leckiego   wli\n",
            "Bent       #gro           #polskiego  #zy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### WVO Interpretation\n",
        " polega na analizie macierzy przejścia, identyfikując pary słów, które są silnie ze sobą powiązane."
      ],
      "metadata": {
        "id": "Y2Rm9PzPuJOy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "prefiksy"
      ],
      "metadata": {
        "id": "x2flGSQ1joc3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "i1, i2 = 21, 7\n",
        "\n",
        "# Move tensors to the selected device\n",
        "W_V_tmp = W_V_heads[i1, i2, :].to(device)\n",
        "W_O_tmp = W_O_heads[i1, i2].to(device)\n",
        "emb_inv = emb_inv.to(device)\n",
        "emb = emb.to(device)\n",
        "\n",
        "# Perform the computation on the device\n",
        "tmp = (emb_inv @ (W_V_tmp @ W_O_tmp) @ emb)\n",
        "all_high_pos = approx_topk(tmp, th0=1, verbose=True)\n",
        "get_top_entries(tmp, all_high_pos, only_ascii=only_ascii, only_alnum=only_alnum,\n",
        "                exclude_same=exclude_same, tokens_list=None)"
      ],
      "metadata": {
        "id": "Noajz1QXIKBa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "822d4abd-a140-4474-f589-d9b5ab2abe8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(' ranem', ' nad'),\n",
              " ('ornie', ' przez'),\n",
              " ('datek', ' nad'),\n",
              " ('arl', ' przed'),\n",
              " (' pewno', ' na'),\n",
              " ('spodziewanie', ' nad'),\n",
              " (' razu', ' od'),\n",
              " ('miernie', ' nad'),\n",
              " (' okazji', ' przy'),\n",
              " (' wsk', ' na'),\n",
              " (' czele', ' na'),\n",
              " ('orne', ' przez'),\n",
              " ('godziny', ' nad'),\n",
              " ('sione', ' przed'),\n",
              " (' ranem', 'nad'),\n",
              " ('natural', ' nad'),\n",
              " ('przewo', ' nad'),\n",
              " (' Duna', ' nad'),\n",
              " ('ktory', ' do'),\n",
              " ('miar', ' nad'),\n",
              " (' dobra', ' dla'),\n",
              " (' Jezi', ' nad'),\n",
              " ('spodzie', ' nad'),\n",
              " (' niedawna', ' do'),\n",
              " ('pisie', ' pod'),\n",
              " (' wygody', ' dla'),\n",
              " ('arcie', ' przed'),\n",
              " (' sumie', ' w'),\n",
              " ('miernie', 'nad'),\n",
              " ('tek', ' pod'),\n",
              " ('wcze', ' przed'),\n",
              " ('mier', ' nad'),\n",
              " ('hala', ' pod'),\n",
              " ('ornie', ' przeze'),\n",
              " (' uboczu', ' na'),\n",
              " (' podstawie', ' na'),\n",
              " (' ranem', 'Nad'),\n",
              " ('czesne', ' do'),\n",
              " ('granicznych', ' nad'),\n",
              " ('wiska', ' przez'),\n",
              " (' barkach', ' na'),\n",
              " ('ornie', 'przez'),\n",
              " (' odmiany', ' dla'),\n",
              " ('niego', ' przed'),\n",
              " (' plecami', ' za'),\n",
              " (' dobi', ' na'),\n",
              " ('przewodni', ' nad'),\n",
              " (' koniec', ' pod'),\n",
              " (' razie', ' na'),\n",
              " (' potrzeby', ' na')]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i1, i2 = 20, 13\n",
        "\n",
        "# Move tensors to the selected device\n",
        "W_V_tmp = W_V_heads[i1, i2, :].to(device)\n",
        "W_O_tmp = W_O_heads[i1, i2].to(device)\n",
        "emb_inv = emb_inv.to(device)\n",
        "emb = emb.to(device)\n",
        "\n",
        "# Perform the computation on the device\n",
        "tmp = (emb_inv @ (W_V_tmp @ W_O_tmp) @ emb)\n",
        "all_high_pos = approx_topk(\n",
        "    tmp, th0=1, verbose=True\n",
        ")  # torch.nonzero((tmp > th) & (tmp < th_max)).tolist()\n",
        "\n",
        "get_top_entries(\n",
        "    tmp,\n",
        "    all_high_pos,\n",
        "    only_ascii=only_ascii,\n",
        "    only_alnum=only_alnum,\n",
        "    exclude_same=exclude_same,\n",
        "    tokens_list=None,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBtk9MjfQkw8",
        "outputId": "92854a5a-2737-488a-c47a-3b9c9fcddc1d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "one more iteration. 358\n",
            "one more iteration. 11868\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(' jej', ' jej'),\n",
              " (' jej', ' ona'),\n",
              " (' Ciebie', ' Ciebie'),\n",
              " (' jej', ' Ona'),\n",
              " (' wjej', ' jej'),\n",
              " (' Ciebie', ' Tobie'),\n",
              " (' ciebie', ' ciebie'),\n",
              " (' Twoich', ' Ciebie'),\n",
              " (' ich', ' ich'),\n",
              " (' ciebie', ' tobie'),\n",
              " (' Ciebie', ' Twojej'),\n",
              " (' Twojej', ' Ciebie'),\n",
              " (' Ciebie', ' Twoich'),\n",
              " (' twoimi', ' ciebie'),\n",
              " (' wjej', ' ona'),\n",
              " (' Twoim', ' Ciebie'),\n",
              " (' Ciebie', ' Twoim'),\n",
              " (' twoich', ' ciebie'),\n",
              " (' Twoje', ' Ciebie'),\n",
              " (' twoje', ' ciebie'),\n",
              " (' jej', ' wjej'),\n",
              " (' Ciebie', ' Twoje'),\n",
              " (' Twoich', ' Tobie'),\n",
              " (' wjej', ' Ona'),\n",
              " (' Tobie', ' Ciebie'),\n",
              " (' Twoich', ' Twoich'),\n",
              " (' ciebie', ' twoich'),\n",
              " (' Twoich', ' Twojej'),\n",
              " (' Ci', ' Ciebie'),\n",
              " (' ciebie', ' twoimi'),\n",
              " (' twoim', ' ciebie'),\n",
              " (' Twoim', ' Twoich'),\n",
              " (' ciebie', ' twojej'),\n",
              " (' was', ' was'),\n",
              " (' ciebie', ' twoim'),\n",
              " (' twoimi', ' tobie'),\n",
              " (' jej', ' niej'),\n",
              " (' jej', ' Jej'),\n",
              " (' was', ' wami'),\n",
              " (' twoich', ' tobie'),\n",
              " (' tobie', ' ciebie'),\n",
              " (' Twoich', ' Twoje'),\n",
              " (' twoich', ' twoich'),\n",
              " (' Twojej', ' Twoich'),\n",
              " (' twoimi', ' twoich'),\n",
              " (' twoje', ' tobie'),\n",
              " (' twoimi', ' twoimi'),\n",
              " (' Twojej', ' Tobie'),\n",
              " (' twojej', ' ciebie'),\n",
              " (' Tobie', ' Tobie')]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### WQK Interpretation\n",
        "Interpretacja Wqk polega na analizie macierzy, identyfikując pary słów, które silnie ze sobą współgrają w kontekście mechanizmu uwagi"
      ],
      "metadata": {
        "id": "px6JYXqvugYF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "imiona"
      ],
      "metadata": {
        "id": "DbxpvFXllT2d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "i1, i2 = 20, 7\n",
        "\n",
        "# Move tensors to the selected device\n",
        "W_Q_tmp = W_Q_heads[i1, i2, :].to(device)\n",
        "W_K_tmp = W_K_heads[i1, i2].to(device)\n",
        "emb_inv = emb_inv.to(device)\n",
        "emb = emb.to(device)\n",
        "\n",
        "tmp = emb_inv @ (W_Q_tmp @ W_K_tmp.T) @ emb_inv.T\n",
        "\n",
        "all_high_pos = approx_topk(tmp, th0=1, verbose=True)\n",
        "\n",
        "get_top_entries(\n",
        "    tmp,\n",
        "    all_high_pos,\n",
        "    only_ascii=only_ascii,\n",
        "    only_alnum=only_alnum,\n",
        "    exclude_same=exclude_same,\n",
        "    tokens_list=None,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbBcbUz6kHv5",
        "outputId": "fdb4dbeb-8d7b-4d84-eb03-298f7241393a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "one more iteration. 0\n",
            "one more iteration. 0\n",
            "one more iteration. 580\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(' Clark', ' Clark'),\n",
              " (' Allen', ' Allen'),\n",
              " (' Marie', ' Marie'),\n",
              " (' Anderson', ' Anderson'),\n",
              " (' Haw', 'sau'),\n",
              " (' Taylor', ' Taylor'),\n",
              " (' Betty', ' Bell'),\n",
              " (' Betty', ' Brown'),\n",
              " (' Jones', ' Jones'),\n",
              " (' Lucas', ' Lucas'),\n",
              " (' Sally', 'yn'),\n",
              " (' Adams', ' Adams'),\n",
              " (' Thom', ' Thom'),\n",
              " (' Nicole', ' Anderson'),\n",
              " (' Bent', ' Bent'),\n",
              " (' Davi', ' Jones'),\n",
              " (' Claire', ' Claire'),\n",
              " (' Jone', ' Jones'),\n",
              " (' Morgan', ' Morgan'),\n",
              " (' Laura', 'nifer'),\n",
              " (' Abraham', ' Abraham'),\n",
              " (' Lee', ' Lee'),\n",
              " (' Hanna', ' Hanna'),\n",
              " (' Beck', ' Beck'),\n",
              " (' Blake', ' Blake'),\n",
              " (' Bec', ' Beck'),\n",
              " (' Jane', ' Sim'),\n",
              " (' Kowalskiego', ' Kowalski'),\n",
              " (' Bell', ' Bell'),\n",
              " (' Adams', ' Johnson'),\n",
              " (' Kaczmarek', ' Kaczmarek'),\n",
              " (' Perry', ' Johnson'),\n",
              " (' Betty', ' Miller'),\n",
              " (' Davis', ' Johnson'),\n",
              " (' Julie', ' Julie'),\n",
              " (' Anderson', ' Jones'),\n",
              " (' Betty', ' Case'),\n",
              " (' Daniel', ' Daniel'),\n",
              " (' Davis', ' Anderson'),\n",
              " (' Carter', ' Carter'),\n",
              " (' Hannah', ' Jackson'),\n",
              " (' Clark', ' Johnson'),\n",
              " (' Daniela', ' Daniel'),\n",
              " (' Steve', ' Johnson'),\n",
              " (' Carter', 'the'),\n",
              " (' Molly', 'lyn'),\n",
              " (' Gun', ' Gun'),\n",
              " (' Lewis', ' Williams'),\n",
              " (' Gabriela', 'Gabriel'),\n",
              " (' Ass', ' Ass')]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "pan, panu"
      ],
      "metadata": {
        "id": "BF37dMTjl1Vo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "i1, i2 = 23, 15\n",
        "\n",
        "# Move tensors to the selected device\n",
        "W_Q_tmp = W_Q_heads[i1, i2, :].to(device)\n",
        "W_K_tmp = W_K_heads[i1, i2].to(device)\n",
        "emb_inv = emb_inv.to(device)\n",
        "emb = emb.to(device)\n",
        "\n",
        "tmp = emb_inv @ (W_Q_tmp @ W_K_tmp.T) @ emb_inv.T\n",
        "\n",
        "all_high_pos = approx_topk(tmp, th0=1, verbose=True)\n",
        "\n",
        "get_top_entries(\n",
        "    tmp,\n",
        "    all_high_pos,\n",
        "    only_ascii=only_ascii,\n",
        "    only_alnum=only_alnum,\n",
        "    exclude_same=exclude_same,\n",
        "    tokens_list=None,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjrC2F1_leIq",
        "outputId": "5ea2a30c-830d-49ca-f32d-d0676f7cc3fd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "one more iteration. 0\n",
            "one more iteration. 573\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(' panem', ' pan'),\n",
              " (' pana', ' pan'),\n",
              " (' pan', ' panu'),\n",
              " (' pana', ' panu'),\n",
              " (' panem', ' panu'),\n",
              " (' pan', ' pan'),\n",
              " (' panem', ' pana'),\n",
              " (' pana', ' pana'),\n",
              " (' pan', ' pana'),\n",
              " (' panu', ' panu'),\n",
              " (' Pani', ' Pani'),\n",
              " ('Pana', ' pan'),\n",
              " (' pana', ' Panu'),\n",
              " (' pan', ' Panu'),\n",
              " (' panu', ' pan'),\n",
              " ('panem', ' pan'),\n",
              " (' Panem', ' pan'),\n",
              " (' pana', ' Pan'),\n",
              " (' pani', ' Pani'),\n",
              " (' pana', 'pan'),\n",
              " (' pana', ' panowie'),\n",
              " (' panem', ' Pan'),\n",
              " (' pan', 'pan'),\n",
              " (' pan', ' panem'),\n",
              " (' panu', ' pana'),\n",
              " (' pan', ' Pan'),\n",
              " ('Pani', ' pani'),\n",
              " (' pana', ' Pana'),\n",
              " ('Pani', ' Pani'),\n",
              " ('panem', ' panu'),\n",
              " (' pan', ' Pana'),\n",
              " (' panem', ' Panu'),\n",
              " (' pana', ' panem'),\n",
              " (' pan', ' panom'),\n",
              " ('Pana', ' pana'),\n",
              " ('Pana', ' panu'),\n",
              " (' Pana', ' pan'),\n",
              " ('Pan', ' pan'),\n",
              " (' panom', ' pan'),\n",
              " ('Pan', ' pana'),\n",
              " (' Panem', ' panu'),\n",
              " (' Panem', ' pana'),\n",
              " (' panem', ' Pana'),\n",
              " (' pani', ' pani'),\n",
              " (' pana', ' panom'),\n",
              " (' panem', ' panem'),\n",
              " ('panem', ' pana'),\n",
              " ('Pan', ' panu'),\n",
              " (' panu', ' Panu'),\n",
              " (' Panu', ' pan')]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT"
      ],
      "metadata": {
        "id": "H8ukFqKmuwLR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sacremoses"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQslDOhnn1gO",
        "outputId": "849020a0-4989-403d-f2f2-37bc2687e1b2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.10/dist-packages (0.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacremoses) (2024.9.11)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.4.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sacremoses) (4.66.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\"allegro/herbert-base-cased\")\n",
        "tokenizer = my_tokenizer = AutoTokenizer.from_pretrained(\"allegro/herbert-base-cased\")\n",
        "emb = model.get_output_embeddings().weight.data.T.detach()\n",
        "model.config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCpuZvuRl7ck",
        "outputId": "3696e038-f512-40a4-9db8-df27c6296136"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertConfig {\n",
              "  \"_attn_implementation_autoset\": true,\n",
              "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
              "  \"architectures\": [\n",
              "    \"BertModel\"\n",
              "  ],\n",
              "  \"attention_probs_dropout_prob\": 0.1,\n",
              "  \"classifier_dropout\": null,\n",
              "  \"hidden_act\": \"gelu\",\n",
              "  \"hidden_dropout_prob\": 0.1,\n",
              "  \"hidden_size\": 768,\n",
              "  \"initializer_range\": 0.02,\n",
              "  \"intermediate_size\": 3072,\n",
              "  \"layer_norm_eps\": 1e-12,\n",
              "  \"max_position_embeddings\": 514,\n",
              "  \"model_type\": \"bert\",\n",
              "  \"num_attention_heads\": 12,\n",
              "  \"num_hidden_layers\": 12,\n",
              "  \"pad_token_id\": 1,\n",
              "  \"position_embedding_type\": \"absolute\",\n",
              "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
              "  \"transformers_version\": \"4.46.2\",\n",
              "  \"type_vocab_size\": 2,\n",
              "  \"use_cache\": true,\n",
              "  \"vocab_size\": 50000\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ekstrakcja wag z modelu BERT\n"
      ],
      "metadata": {
        "id": "eHq1C0H_uyLf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_layers = model.config.num_hidden_layers\n",
        "num_heads = model.config.num_attention_heads\n",
        "hidden_dim = model.config.hidden_size\n",
        "head_size = hidden_dim // num_heads"
      ],
      "metadata": {
        "id": "gwa7dwdcosUU"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(num_layers)\n",
        "print(num_heads)\n",
        "print(hidden_dim)\n",
        "print(head_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asOCnQt6otgn",
        "outputId": "e7a6e436-77f0-4497-a913-6536fc52e523"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12\n",
            "12\n",
            "768\n",
            "64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "K = torch.cat(\n",
        "    [\n",
        "        model.get_parameter(f\"bert.encoder.layer.{j}.attention.self.key.weight\").T\n",
        "        for j in range(num_layers)\n",
        "    ]\n",
        ").detach()\n",
        "V = torch.cat(\n",
        "    [\n",
        "        model.get_parameter(f\"bert.encoder.layer.{j}.attention.self.value.weight\")\n",
        "        for j in range(num_layers)\n",
        "    ]\n",
        ").detach()\n",
        "\n",
        "W_Q, W_K, W_V = (\n",
        "    torch.cat(\n",
        "        [\n",
        "            model.get_parameter(f\"bert.encoder.layer.{j}.attention.self.query.weight\")\n",
        "            for j in range(num_layers)\n",
        "        ]\n",
        "    )\n",
        "    .detach()\n",
        "    .chunk(3, dim=-1)\n",
        ")\n",
        "W_O = torch.cat(\n",
        "    [\n",
        "        model.get_parameter(f\"bert.encoder.layer.{j}.attention.output.dense.weight\")\n",
        "        for j in range(num_layers)\n",
        "    ]\n",
        ").detach()\n",
        "\n",
        "K_heads = K.reshape(num_layers, -1, hidden_dim)\n",
        "V_heads = V.reshape(num_layers, -1, hidden_dim)\n",
        "d_int = K_heads.shape[1]\n",
        "\n",
        "W_Q_heads = [model.get_parameter(f\"bert.encoder.layer.{j}.attention.self.query.weight\").detach() for j in range(num_layers)]\n",
        "W_Q_heads = [w.reshape(hidden_dim, num_heads, head_size).permute(1, 0, 2) for w in W_Q_heads]  # Reshape and permute each layer's weights separately\n",
        "\n",
        "W_K_heads = [model.get_parameter(f\"bert.encoder.layer.{j}.attention.self.key.weight\").detach() for j in range(num_layers)]\n",
        "W_K_heads = [w.reshape(hidden_dim, num_heads, head_size).permute(1, 0, 2) for w in W_K_heads]\n",
        "\n",
        "W_V_heads = [model.get_parameter(f\"bert.encoder.layer.{j}.attention.self.value.weight\").detach() for j in range(num_layers)]\n",
        "W_V_heads = [w.reshape(hidden_dim, num_heads, head_size).permute(1, 0, 2) for w in W_V_heads]\n",
        "\n",
        "W_O_heads = [model.get_parameter(f\"bert.encoder.layer.{j}.attention.output.dense.weight\").detach() for j in range(num_layers)]\n",
        "W_O_heads = [w.reshape(num_heads, head_size, hidden_dim) for w in W_O_heads]\n",
        "emb_inv = emb.T\n",
        "print(emb_inv.shape)\n",
        "print(f\"Layers in the model: {num_layers}\")\n",
        "print(f\"Neurons in the model: {model.config.hidden_size}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tB5miuXEl3vy",
        "outputId": "f364475f-e691-4760-ded3-79249a8685ee"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([50000, 768])\n",
            "Layers in the model: 12\n",
            "Neurons in the model: 768\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "W_Q_heads = torch.stack([model.get_parameter(f\"bert.encoder.layer.{j}.attention.self.query.weight\").detach().reshape(hidden_dim, num_heads, head_size).permute(1, 0, 2) for j in range(num_layers)])\n",
        "W_K_heads = torch.stack([model.get_parameter(f\"bert.encoder.layer.{j}.attention.self.key.weight\").detach().reshape(hidden_dim, num_heads, head_size).permute(1, 0, 2) for j in range(num_layers)])\n",
        "W_V_heads = torch.stack([model.get_parameter(f\"bert.encoder.layer.{j}.attention.self.value.weight\").detach().reshape(hidden_dim, num_heads, head_size).permute(1, 0, 2) for j in range(num_layers)])\n",
        "W_O_heads = torch.stack([model.get_parameter(f\"bert.encoder.layer.{j}.attention.output.dense.weight\").detach().reshape(num_heads, head_size, hidden_dim) for j in range(num_layers)])"
      ],
      "metadata": {
        "id": "bZ_xtUO9qf7a"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(K_heads.shape)\n",
        "print(V_heads.shape)\n",
        "print(W_Q_heads.shape)\n",
        "print(W_K_heads.shape)\n",
        "print(W_V_heads.shape)\n",
        "print(W_O_heads.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yS_q8vR6p6GB",
        "outputId": "412d8536-33ff-48fa-e6ee-390c9ed08a9f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([12, 768, 768])\n",
            "torch.Size([12, 768, 768])\n",
            "torch.Size([12, 12, 768, 64])\n",
            "torch.Size([12, 12, 768, 64])\n",
            "torch.Size([12, 12, 768, 64])\n",
            "torch.Size([12, 12, 64, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interpretacja wag modelu BERT na pustej liście tokenów"
      ],
      "metadata": {
        "id": "QNyILB2EvBir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_list = set()"
      ],
      "metadata": {
        "id": "MmiQUykAqjdw"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ilayer = 10\n",
        "ineuron = 55\n",
        "print_top_tokens(ilayer, ineuron, K_heads, V_heads, emb, tokens_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W360C1fRqqp5",
        "outputId": "f22dfdc8-1797-48de-8322-8939bf5725ae"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer 10, Neuron 55\n",
            "K                 V               -K                 -V\n",
            "----------------  --------------  -----------------  ------------------\n",
            "#targ             #tyn            #vio</w>           #repli\n",
            "#komplet          #tyn</w>        #Dla</w>           #wyrobi\n",
            "#targ</w>         #kontyn         #DLA</w>           #serie</w>\n",
            "#table            #sztyn</w>      #ROZ               #018</w>\n",
            "#komplet</w>      #cykl           #dobro</w>         #dora\n",
            "#kura             #komitet</w>    #Ze</w>            #interpreta\n",
            "#Ori              #zony</w>       #lizuje</w>        #suser</w>\n",
            "#kin</w>          #iny</w>        #noszone</w>       #empi\n",
            "#kongre           #Komitet</w>    #dla</w>           #007</w>\n",
            "#Litera           #ranka</w>      #RIO</w>           #Serie</w>\n",
            "#~                #tabu</w>       #Ade               #rece\n",
            "#komp             #fotel</w>      #oddania</w>       #interpretacja</w>\n",
            "#Kajetan</w>      #dzieckiem</w>  #wykorzystuje</w>  #lera</w>\n",
            "#instan           #rytm</w>       #Dobro             #Wyro\n",
            "#shad             #tlen           #dobro             #narzu\n",
            "#KN</w>           #tany</w>       #podzielone</w>    #hrabia</w>\n",
            "#wet</w>          #ety            #Szacuje</w>       #wymieni\n",
            "#plakat</w>       #cic</w>        #podniesienia</w>  #intro\n",
            "#komputerowy</w>  #tlen</w>       #niesienia</w>     #wyro\n",
            "#mno              #nocleg</w>     #walory</w>        #inaugura\n",
            "#rect</w>         #internetu</w>  #dzenie</w>        #ekspor\n",
            "#^                #Cykl</w>       #szeroko</w>       #surow\n",
            "#niczu</w>        #lko</w>        #liwi</w>          #impor\n",
            "#czu</w>          #kafe           #dowi              #interpre\n",
            "#komputerowa</w>  #CIN</w>        #IA</w>            #elimina\n",
            "#czowi</w>        #Kol            #niam</w>          #prepara\n",
            "#zasadzie</w>     #piln           #UNE               #opere\n",
            "#analog           #kilometr</w>   #towanie</w>       #wypar\n",
            "#koni             #cznym</w>      #walory            #mura</w>\n",
            "#Komputer</w>     #czkiem</w>     #lokale</w>        #wri\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ilayer = 7\n",
        "ineuron = 515\n",
        "print_top_tokens(ilayer, ineuron, K_heads, V_heads, emb, tokens_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FD_Wx7Irjxe",
        "outputId": "2b495086-f117-4ad8-ff28-041913e743f5"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer 7, Neuron 515\n",
            "K                V                  -K                -V\n",
            "---------------  -----------------  ----------------  ------------\n",
            "#norma</w>       #uczucie</w>       #rodu</w>         #|\n",
            "#norma           #powstanie</w>     #nienia</w>       #monet</w>\n",
            "#NHL</w>         #relacji</w>       #kow</w>          #MIGA</w>\n",
            "#nota</w>        #swego</w>         #alternatyw       #nn\n",
            "#oma</w>         #drugiego</w>      #prosimy</w>      #ALU\n",
            "#smal            #dziewi            #gwiazd           #nal</w>\n",
            "#dag</w>         #drugiej</w>       #spraw</w>        #tyn\n",
            "#mist            #pierwszego</w>    #nian</w>         #Marti\n",
            "#chorymi</w>     #drzwi             #laz              #IO</w>\n",
            "#Hanna</w>       #tago              #czytaj</w>       #wyd</w>\n",
            "#trol            #prv</w>           #lania</w>        #Journal</w>\n",
            "#Tisch           #oznacza</w>       #wienia</w>       #ener\n",
            "#nota            #modeli</w>        #plaz             #NOW\n",
            "#Monte           #prawda</w>        #kowej</w>        #zio\n",
            "#Geo             #mojego</w>        #Jed              #ctwa</w>\n",
            "#Filo            #mojej</w>         #spraw            #ule\n",
            "#int</w>         #puszczenie</w>    #bier             #termo\n",
            "#MY              #wyboru</w>        #alnych</w>       #idio\n",
            "#ymi</w>         #praktyczne</w>    #nienie</w>       #nego\n",
            "#XP</w>          #najmniejszej</w>  #kowych</w>       #parkin\n",
            "#minera          #kwestii</w>       #szereg           #hospita\n",
            "#Wincentego</w>  #swojego</w>       #dostaw</w>       #gos</w>\n",
            "#parafia</w>     #takiego</w>       #kowy             #Bezdom\n",
            "#pem</w>         #polskiej</w>      #istnienia</w>    #Inform\n",
            "#kilometr</w>    #niemieckiego</w>  #kowa</w>         #cau\n",
            "#lega</w>        #idzie</w>         #kow              #ic\n",
            "#mymi</w>        #kierunku</w>      #fer              #repertua\n",
            "#antro           #koniec</w>        #lad              #raju</w>\n",
            "#normami</w>     #otwarte</w>       #Manche           #Epidemi\n",
            "#reforma</w>     #pierwsze          #komunikacji</w>  #lio</w>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### WVO Interpretation"
      ],
      "metadata": {
        "id": "0Zjlza_WvLFm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "liczby"
      ],
      "metadata": {
        "id": "EOFjLo8qsDbR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "i1, i2 = 11, 7\n",
        "\n",
        "# Move tensors to the selected device\n",
        "W_V_tmp = W_V_heads[i1, i2, :].to(device)\n",
        "W_O_tmp = W_O_heads[i1, i2].to(device)\n",
        "emb_inv = emb_inv.to(device)\n",
        "emb = emb.to(device)\n",
        "\n",
        "# Perform the computation on the device\n",
        "tmp = (emb_inv @ (W_V_tmp @ W_O_tmp) @ emb)\n",
        "all_high_pos = approx_topk(tmp, th0=1, verbose=True)\n",
        "get_top_entries(tmp, all_high_pos, only_ascii=only_ascii, only_alnum=only_alnum,\n",
        "                exclude_same=exclude_same, tokens_list=None)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dx3qVXAortJW",
        "outputId": "df811024-293c-45d4-971a-ed10f1215d8f"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "one more iteration. 198449126\n",
            "one more iteration. 1006820\n",
            "one more iteration. 43\n",
            "one more iteration. 6722\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('351', '355'),\n",
              " ('351', '351'),\n",
              " ('351', '349'),\n",
              " ('rale', 'rale'),\n",
              " ('Dort', 'Dort'),\n",
              " ('351', '353'),\n",
              " ('arium', 'arium'),\n",
              " ('355', '355'),\n",
              " ('353', '353'),\n",
              " ('389', '389'),\n",
              " ('nawi', 'nawi'),\n",
              " ('tlu', 'tlu'),\n",
              " ('367', '353'),\n",
              " ('367', '349'),\n",
              " ('DRA', 'DRA'),\n",
              " ('1908', '1910'),\n",
              " ('351', 'fanta'),\n",
              " ('389', '355'),\n",
              " ('363', '363'),\n",
              " ('367', '355'),\n",
              " ('353', '351'),\n",
              " ('1905', '1904'),\n",
              " ('351', '345'),\n",
              " ('367', '332'),\n",
              " ('367', '339'),\n",
              " ('435', '332'),\n",
              " ('367', '880'),\n",
              " ('368', '353'),\n",
              " ('435', '435'),\n",
              " ('334', '332'),\n",
              " ('353', '355'),\n",
              " ('439', '349'),\n",
              " ('363', '353'),\n",
              " ('351', '332'),\n",
              " ('387', '355'),\n",
              " ('435', '349'),\n",
              " ('1908', '1904'),\n",
              " ('scher', 'scher'),\n",
              " ('439', '332'),\n",
              " ('332', '332'),\n",
              " ('435', '355'),\n",
              " ('1899', '1949'),\n",
              " ('439', '353'),\n",
              " ('1905', '1910'),\n",
              " ('368', '332'),\n",
              " ('1908', '1925'),\n",
              " ('439', '435'),\n",
              " ('1890', '1904'),\n",
              " ('439', '647'),\n",
              " ('389', '349')]"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i1, i2 = 4, 5\n",
        "\n",
        "# Move tensors to the selected device\n",
        "W_V_tmp = W_V_heads[i1, i2, :].to(device)\n",
        "W_O_tmp = W_O_heads[i1, i2].to(device)\n",
        "emb_inv = emb_inv.to(device)\n",
        "emb = emb.to(device)\n",
        "\n",
        "# Perform the computation on the device\n",
        "tmp = (emb_inv @ (W_V_tmp @ W_O_tmp) @ emb)\n",
        "all_high_pos = approx_topk(tmp, th0=1, verbose=True)\n",
        "get_top_entries(tmp, all_high_pos, only_ascii=only_ascii, only_alnum=only_alnum,\n",
        "                exclude_same=exclude_same, tokens_list=None)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0IMkNMCsFpB",
        "outputId": "83080c5e-64cf-4a50-c811-05bbdc7a9d11"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "one more iteration. 1614823723\n",
            "one more iteration. 531785098\n",
            "one more iteration. 8901907\n",
            "one more iteration. 473\n",
            "one more iteration. 67368\n",
            "one more iteration. 5742\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('obrotach', 'remis'),\n",
              " ('obrotach', 'remis'),\n",
              " ('obrotach', 'DIE'),\n",
              " ('iera', 'Spo'),\n",
              " ('zaka', 'Gad'),\n",
              " ('dzo', 'blon'),\n",
              " ('iera', 'ATP'),\n",
              " ('dzo', 'jury'),\n",
              " ('sic', 'blon'),\n",
              " ('iera', 'WWW'),\n",
              " ('iera', 'Bou'),\n",
              " ('dzo', 'spro'),\n",
              " ('dych', 'remis'),\n",
              " ('iera', 'obie'),\n",
              " ('iera', 'ABC'),\n",
              " ('sic', 'blon'),\n",
              " ('iera', 'jury'),\n",
              " ('dych', 'remis'),\n",
              " ('obrotach', 'Spo'),\n",
              " ('iera', 'PIL'),\n",
              " ('trol', 'Sot'),\n",
              " ('nowa', 'rej'),\n",
              " ('obrotach', 'KAN'),\n",
              " ('dych', 'DIE'),\n",
              " ('zaka', 'remis'),\n",
              " ('dzo', 'kapu'),\n",
              " ('obrotach', 'ABC'),\n",
              " ('iera', 'Eva'),\n",
              " ('iera', 'plot'),\n",
              " ('iera', 'Cham'),\n",
              " ('dzo', 'rej'),\n",
              " ('dzo', 'blon'),\n",
              " ('Magiera', 'WF'),\n",
              " ('upiera', 'Spo'),\n",
              " ('dzo', 'Sot'),\n",
              " ('iera', 'EK'),\n",
              " ('dzo', 'Pot'),\n",
              " ('iera', 'VE'),\n",
              " ('iera', 'domu'),\n",
              " ('Magiera', 'WWW'),\n",
              " ('lew', 'remis'),\n",
              " ('iera', 'fax'),\n",
              " ('dych', 'rej'),\n",
              " ('iera', 'Spy'),\n",
              " ('dzo', 'ABC'),\n",
              " ('iera', 'niebie'),\n",
              " ('iera', 'formule'),\n",
              " ('upiera', 'WF'),\n",
              " ('lew', 'remis'),\n",
              " ('iera', 'bie')]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### WKQ Interpretation"
      ],
      "metadata": {
        "id": "BJWktR6rvNFK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "i1, i2 = 2, 2\n",
        "\n",
        "# Move tensors to the selected device\n",
        "W_Q_tmp = W_Q_heads[i1, i2, :].to(device)\n",
        "W_K_tmp = W_K_heads[i1, i2].to(device)\n",
        "emb_inv = emb_inv.to(device)\n",
        "emb = emb.to(device)\n",
        "\n",
        "tmp = emb_inv @ (W_Q_tmp @ W_K_tmp.T) @ emb_inv.T\n",
        "\n",
        "all_high_pos = approx_topk(tmp, th0=1, verbose=True)\n",
        "\n",
        "get_top_entries(\n",
        "    tmp,\n",
        "    all_high_pos,\n",
        "    only_ascii=only_ascii,\n",
        "    only_alnum=only_alnum,\n",
        "    exclude_same=exclude_same,\n",
        "    tokens_list=None,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KIVLXFestwZ",
        "outputId": "3188c007-7e3c-496d-abe3-2bac00ae442f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "one more iteration. 799290475\n",
            "one more iteration. 315156187\n",
            "one more iteration. 21015350\n",
            "one more iteration. 17873\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Ferenc', 'napi'),\n",
              " ('Lista', 'NEJ'),\n",
              " ('Transport', 'Bom'),\n",
              " ('owali', 'Canal'),\n",
              " ('Hos', 'kow'),\n",
              " ('Natura', 'NEJ'),\n",
              " ('Ferenc', 'FN'),\n",
              " ('Wen', 'Bom'),\n",
              " ('Ferenc', 'Kru'),\n",
              " ('Lista', 'kow'),\n",
              " ('bra', 'NEJ'),\n",
              " ('Stel', 'Bom'),\n",
              " ('Kluby', 'kow'),\n",
              " ('Juli', 'Bom'),\n",
              " ('barba', 'KTU'),\n",
              " ('publicznego', 'napi'),\n",
              " ('Br', 'NEJ'),\n",
              " ('pry', 'NEJ'),\n",
              " ('Lista', 'Arme'),\n",
              " ('NEGO', 'KOV'),\n",
              " ('Ferenc', 'Google'),\n",
              " ('pobo', 'NEJ'),\n",
              " ('Pry', 'NEJ'),\n",
              " ('Ele', 'Tol'),\n",
              " ('Infra', 'Bom'),\n",
              " ('Andrew', 'NEJ'),\n",
              " ('Infra', 'Arme'),\n",
              " ('Administracyjnego', 'znie'),\n",
              " ('Dos', 'znacznej'),\n",
              " ('Ferenc', 'Canal'),\n",
              " ('Wen', 'NEJ'),\n",
              " ('Lesznie', 'znie'),\n",
              " ('Ferenc', 'Czech'),\n",
              " ('Juli', 'kow'),\n",
              " ('Indo', 'KOV'),\n",
              " ('Lista', 'Tol'),\n",
              " ('Kuba', 'NEJ'),\n",
              " ('Mario', 'Tol'),\n",
              " ('poda', 'kow'),\n",
              " ('Tol', 'doby'),\n",
              " ('Kuba', 'kow'),\n",
              " ('publicznego', 'kowy'),\n",
              " ('Kore', 'NEJ'),\n",
              " ('Instru', 'Arme'),\n",
              " ('Hos', 'cyjnych'),\n",
              " ('Klu', 'FN'),\n",
              " ('Fle', 'kow'),\n",
              " ('Ferenc', 'Hy'),\n",
              " ('Dos', 'kow'),\n",
              " ('bra', 'NY')]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i1, i2 = 8, 11\n",
        "\n",
        "# Move tensors to the selected device\n",
        "W_Q_tmp = W_Q_heads[i1, i2, :].to(device)\n",
        "W_K_tmp = W_K_heads[i1, i2].to(device)\n",
        "emb_inv = emb_inv.to(device)\n",
        "emb = emb.to(device)\n",
        "\n",
        "tmp = emb_inv @ (W_Q_tmp @ W_K_tmp.T) @ emb_inv.T\n",
        "\n",
        "all_high_pos = approx_topk(tmp, th0=1, verbose=True)\n",
        "\n",
        "get_top_entries(\n",
        "    tmp,\n",
        "    all_high_pos,\n",
        "    only_ascii=only_ascii,\n",
        "    only_alnum=only_alnum,\n",
        "    exclude_same=exclude_same,\n",
        "    tokens_list=None,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHFO4GpQtGGm",
        "outputId": "100a75a0-cc19-43f8-d00f-30bc63ad538f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "one more iteration. 464473585\n",
            "one more iteration. 99335701\n",
            "one more iteration. 1815857\n",
            "one more iteration. 249\n",
            "one more iteration. 21559\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('analogi', 'mentar'),\n",
              " ('Wiele', 'Wiele'),\n",
              " ('275', 'Wiele'),\n",
              " ('192', 'Red'),\n",
              " ('275', '649'),\n",
              " ('512', 'Wiele'),\n",
              " ('alne', '256'),\n",
              " ('192', '384'),\n",
              " ('Wiele', 'Tere'),\n",
              " ('kod', 'Pur'),\n",
              " ('alne', '384'),\n",
              " ('192', 'Leszczy'),\n",
              " ('275', '384'),\n",
              " ('299', 'Wiele'),\n",
              " ('384', '649'),\n",
              " ('wyraz', 'tenis'),\n",
              " ('299', '512'),\n",
              " ('512', 'Leszczy'),\n",
              " ('512', 'Red'),\n",
              " ('512', '384'),\n",
              " ('312', 'Wiele'),\n",
              " ('192', '512'),\n",
              " ('275', '249'),\n",
              " ('kod', 'Tere'),\n",
              " ('192', 'Lite'),\n",
              " ('wyraz', 'najem'),\n",
              " ('baj', '512'),\n",
              " ('Rzecz', 'Stel'),\n",
              " ('mno', '512'),\n",
              " ('384', '384'),\n",
              " ('Medal', 'statystyk'),\n",
              " ('wyraz', 'Interna'),\n",
              " ('zej', 'ction'),\n",
              " ('baj', '256'),\n",
              " ('portal', 'tional'),\n",
              " ('297', 'Wiele'),\n",
              " ('wyraz', 'cke'),\n",
              " ('256', '649'),\n",
              " ('249', '649'),\n",
              " ('512', 'Fle'),\n",
              " ('332', 'Wiele'),\n",
              " ('275', 'Fle'),\n",
              " ('512', '431'),\n",
              " ('254', '649'),\n",
              " ('godziny', 'banko'),\n",
              " ('299', 'Red'),\n",
              " ('najmniejszego', '384'),\n",
              " ('242', 'Red'),\n",
              " ('192', '192'),\n",
              " ('alne', 'Wiele')]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    }
  ]
}