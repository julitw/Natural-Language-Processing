{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVUtaieOgWCa"
      },
      "outputs": [],
      "source": [
        "pip install -U langchain langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJCWIYaq-rvx"
      },
      "outputs": [],
      "source": [
        "pip install -U langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_AopdwVv_nac"
      },
      "outputs": [],
      "source": [
        "pip install unstructured"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40jH5wq8oUg5"
      },
      "outputs": [],
      "source": [
        "!pip install langchain_openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StvbIp0ZLFE1"
      },
      "outputs": [],
      "source": [
        "pip install \"unstructured[md]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "YC_TnY7O-hyN"
      },
      "outputs": [],
      "source": [
        "import langchain\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "from langchain.prompts import ChatPromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "am20OAuW_QM_"
      },
      "outputs": [],
      "source": [
        "def load_documents():\n",
        "  loader = DirectoryLoader('books/')\n",
        "  documents = loader.load()\n",
        "  return documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "8fmmwfG__doR"
      },
      "outputs": [],
      "source": [
        "documents = load_documents()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "joUoghkPa0eK"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "from langchain.schema import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "def split_text(documents: List[Document]):\n",
        "  text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=500, length_function=len, add_start_index=True)\n",
        "  chunks = text_splitter.split_documents(documents)\n",
        "  print(f\"Split {len(documents)} documents into {len(chunks)} chunks.\")\n",
        "\n",
        "\n",
        "  return chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "wMCr9FVlhtEQ"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "openai_api_key = \"uiXn64-EBi__XOmb9n0W4xExkcT_iZBxFGhhHcsHKQ5YLrOs\"\n",
        "openai_api_base = \"https://services.clarin-pl.eu/api/v1/oapi\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "StA-PMGpYG_I",
        "outputId": "e7cb2b6e-2f72-4690-ec82-ce379163393d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/root/.cache/huggingface/modules/transformers_modules/nomic-ai/nomic-bert-2048/eb02ceb48c1fdcc477ff1925c9732c379f0f0d1f/modeling_hf_nomic_bert.py:108: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = loader(resolved_archive_file)\n",
            "WARNING:transformers_modules.nomic-ai.nomic-bert-2048.eb02ceb48c1fdcc477ff1925c9732c379f0f0d1f.modeling_hf_nomic_bert:<All keys matched successfully>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split 1 documents into 120 chunks.\n"
          ]
        }
      ],
      "source": [
        "from langchain.embeddings import SentenceTransformerEmbeddings\n",
        "from langchain_core.vectorstores import InMemoryVectorStore\n",
        "\n",
        "embeddings = SentenceTransformerEmbeddings(model_name=\"nomic-ai/nomic-embed-text-v1\", model_kwargs={\"trust_remote_code\":True})\n",
        "vector_store = InMemoryVectorStore(embeddings)\n",
        "splits = split_text(documents)\n",
        "_ = vector_store.add_documents(documents=splits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "3eg1RUEje5pX"
      },
      "outputs": [],
      "source": [
        "\n",
        "def retrieve(query: str):\n",
        "    \"\"\"Retrieve information related to a query.\"\"\"\n",
        "    retrieved_docs = vector_store.similarity_search(query, k=2)\n",
        "    serialized = \"\\n\\n\".join(\n",
        "        (f\"Content: {doc.page_content}\")\n",
        "        for doc in retrieved_docs\n",
        "    )\n",
        "    sources = \"\\n\\n\".join(\n",
        "        (f\"Sources: {doc.metadata}\")\n",
        "        for doc in retrieved_docs\n",
        "    )\n",
        "    return serialized, sources\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "9Jrvu1mPhRCy"
      },
      "outputs": [],
      "source": [
        "PROMPT_TEMPLATE = \"\"\"\n",
        "Answer the question based only on the following context:\n",
        "\n",
        "{context}\n",
        "\n",
        "---\n",
        "\n",
        "Answer the question based on the above context: {question}\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "E-QoEQ_chNSW"
      },
      "outputs": [],
      "source": [
        "def create_prompt(context_text, query_text):\n",
        "  prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
        "  prompt = prompt_template.format(context=context_text, question=query_text)\n",
        "  return prompt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "dsk2wd9crDXZ"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "client = ChatOpenAI(openai_api_key=openai_api_key, openai_api_base=openai_api_base, model=\"llama\")\n",
        "\n",
        "def generate_response_with_rag(query):\n",
        "  results = retrieve(query)\n",
        "  context_text, sources = results\n",
        "  prompt = create_prompt(context_text, query)\n",
        "  response = client.predict(prompt)\n",
        "  formatted_response = f\"Response: {response}\"\n",
        "  return formatted_response\n",
        "\n",
        "def generate_response(query):\n",
        "  response = client.predict(query)\n",
        "  formatted_response = f\"Response: {response}\"\n",
        "  return formatted_response"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "def print_response(response):\n",
        "  print(textwrap.fill(response, width=80))"
      ],
      "metadata": {
        "id": "oM23PaOfM0J2"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oc7E0ev1r0cz",
        "outputId": "89495480-0f75-498c-e0c1-0cd8cfdf1f45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response: In Lewis Carroll's 'Alice's Adventures in Wonderland', Alice drank a\n",
            "bottle of 'Drink Me' liquid, which tasted like cherry tart, custard, pine-apple,\n",
            "roast turkey, toffee, and buttered toast all mixed together.\n",
            "\n",
            "Response: The flavour of the liquid Alice drank was a mix of cherry-tart,\n",
            "custard, pineapple, roast turkey, toffee, and hot buttered toast.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "query_text = \"What flavour liquid did Alice drink?\"\n",
        "print_response(generate_response(query_text))\n",
        "print()\n",
        "print_response(generate_response_with_rag(query_text))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "query_text = \"How was the white rabbit dressed?\"\n",
        "print_response(generate_response(query_text))\n",
        "print()\n",
        "print_response(generate_response_with_rag(query_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfCnOeCVNBmU",
        "outputId": "0bc7576f-c0fe-48c8-92a7-57fb84e33dee"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response: In Lewis Carroll's 'Alice's Adventures in Wonderland', the White\n",
            "Rabbit is described as wearing a waistcoat.\n",
            "\n",
            "Response: The White Rabbit was \"splendidly\" dressed and was carrying a pair of\n",
            "white kid gloves in one hand and a large fan in the other.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}